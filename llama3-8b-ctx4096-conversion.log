Creating output directory: /Users/felixlin/models/llama-3-8b-Instruct-ctx4096-coreml-fxl/
Checking dependencies...
Checking if macOS version is 15 or higher...
Checking if Python is installed...
Checking if pip is installed...
Checking if coremltools is installed via pip...
Checking if coremlcompiler is available...
Displaying coremlcompiler version...
coremlcompiler version: 3404.23.1
Checking if Python3 is installed...
Checking for model files in the provided directory: /Users/felixlin/models/llama-3-8b-Instruct
Checking for supported architectures in config.json...
All dependencies are satisfied.
Checking dependencies...
Checking if macOS version is 15 or higher...
Checking if Python is installed...
Checking if pip is installed...
Checking if coremltools is installed via pip...
Checking if coremlcompiler is available...
Displaying coremlcompiler version...
coremlcompiler version: 3404.23.1
Checking if Python3 is installed...
Checking for model files in the provided directory: /Users/felixlin/models/llama-3-8b-Instruct
Checking for supported architectures in config.json...
All dependencies are satisfied.
Step 1: Converting Embeddings

Converting model from: /Users/felixlin/models/llama-3-8b-Instruct
Output filename prefix: llama
Batch size: 64
Context length: 4096
Converting part(s): 1
Loading config from /Users/felixlin/models/llama-3-8b-Instruct/config.json

Loaded model config:
  hidden_size: 4096
  vocab_size: 128256
Created lm_head8_1 through lm_head8_8

Loading pretrained weights...
Loading pretrained weights...
Split lm_head weight into lm_head8_1.weight with shape torch.Size([16032, 4096, 1, 1])
Split lm_head weight into lm_head8_2.weight with shape torch.Size([16032, 4096, 1, 1])
Split lm_head weight into lm_head8_3.weight with shape torch.Size([16032, 4096, 1, 1])
Split lm_head weight into lm_head8_4.weight with shape torch.Size([16032, 4096, 1, 1])
Split lm_head weight into lm_head8_5.weight with shape torch.Size([16032, 4096, 1, 1])
Split lm_head weight into lm_head8_6.weight with shape torch.Size([16032, 4096, 1, 1])
Split lm_head weight into lm_head8_7.weight with shape torch.Size([16032, 4096, 1, 1])
Split lm_head weight into lm_head8_8.weight with shape torch.Size([16032, 4096, 1, 1])
Loading model.embed_tokens.weight with shape torch.Size([128256, 4096])
Moving model.embed_tokens.weight to embed_tokens.weight
Missing keys: ['model.kv_cache_0', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.input_layernorm.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.input_layernorm.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.input_layernorm.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.input_layernorm.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.norm.weight']
Reshaped layers.31.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.0.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.0.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.0.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.0.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.0.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.0.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.0.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.1.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.1.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.1.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.1.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.1.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.1.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.1.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.2.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.2.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.2.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.2.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.2.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.2.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.2.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.3.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.3.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.3.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.3.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.3.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.3.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.3.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.4.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.4.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.4.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.4.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.4.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.4.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.4.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.5.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.5.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.5.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.5.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.5.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.5.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.5.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.6.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.6.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.6.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.6.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.6.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.6.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.6.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.7.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.7.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.7.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.7.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.7.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.7.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.7.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.8.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.8.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.8.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.8.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.8.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.8.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.8.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.20.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.20.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.21.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.21.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.21.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.21.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.21.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.21.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.21.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.22.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.22.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.22.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.22.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.22.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.22.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.22.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.23.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.23.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.23.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.23.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.23.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.23.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.23.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.24.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.24.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.24.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.24.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.24.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.24.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.24.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.25.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.25.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.25.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.25.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.25.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.25.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.25.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.26.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.26.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.26.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.26.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.26.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.26.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.26.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.27.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.27.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.27.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.27.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.27.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.27.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.27.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.28.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.28.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.28.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.28.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.28.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.28.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.28.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.29.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.29.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.29.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.29.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.29.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.29.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.29.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.30.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.30.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.30.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.30.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.30.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.30.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.30.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.31.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.31.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.31.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.31.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.31.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.31.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.10.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.10.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.10.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.10.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.10.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.10.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.10.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.11.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.11.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.11.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.11.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.11.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.11.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.11.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.12.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.12.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.12.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.12.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.12.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.12.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.12.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.13.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.13.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.13.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.13.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.13.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.13.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.13.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.14.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.14.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.14.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.14.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.14.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.14.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.14.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.15.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.15.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.15.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.15.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.15.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.15.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.15.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.16.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.16.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.16.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.16.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.16.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.16.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.16.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.17.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.17.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.17.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.17.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.17.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.17.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.17.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.18.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.18.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.18.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.18.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.18.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.18.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.18.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.19.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.19.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.19.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.19.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.19.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.19.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.19.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.20.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.20.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.20.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.20.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.20.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.9.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.9.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.9.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.9.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.9.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.9.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.9.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Pretrained weights loaded successfully
Note: The following expected buffers were initialized:
  - kv_cache_0
  - layers.0.self_attn.rotary_emb.inv_freq
  - layers.1.self_attn.rotary_emb.inv_freq
  - layers.2.self_attn.rotary_emb.inv_freq
  - layers.3.self_attn.rotary_emb.inv_freq
  - layers.4.self_attn.rotary_emb.inv_freq
  - layers.5.self_attn.rotary_emb.inv_freq
  - layers.6.self_attn.rotary_emb.inv_freq
  - layers.7.self_attn.rotary_emb.inv_freq
  - layers.8.self_attn.rotary_emb.inv_freq
  - layers.9.self_attn.rotary_emb.inv_freq
  - layers.10.self_attn.rotary_emb.inv_freq
  - layers.11.self_attn.rotary_emb.inv_freq
  - layers.12.self_attn.rotary_emb.inv_freq
  - layers.13.self_attn.rotary_emb.inv_freq
  - layers.14.self_attn.rotary_emb.inv_freq
  - layers.15.self_attn.rotary_emb.inv_freq
  - layers.16.self_attn.rotary_emb.inv_freq
  - layers.17.self_attn.rotary_emb.inv_freq
  - layers.18.self_attn.rotary_emb.inv_freq
  - layers.19.self_attn.rotary_emb.inv_freq
  - layers.20.self_attn.rotary_emb.inv_freq
  - layers.21.self_attn.rotary_emb.inv_freq
  - layers.22.self_attn.rotary_emb.inv_freq
  - layers.23.self_attn.rotary_emb.inv_freq
  - layers.24.self_attn.rotary_emb.inv_freq
  - layers.25.self_attn.rotary_emb.inv_freq
  - layers.26.self_attn.rotary_emb.inv_freq
  - layers.27.self_attn.rotary_emb.inv_freq
  - layers.28.self_attn.rotary_emb.inv_freq
  - layers.29.self_attn.rotary_emb.inv_freq
  - layers.30.self_attn.rotary_emb.inv_freq
  - layers.31.self_attn.rotary_emb.inv_freq

Converting model part: 1 output_path: llama_embeddings.mlpackage
Preparing model for conversion...
Moving model to device: cpu
Freezing model parameters...
Model preprocessing completed

Converting embeddings layer...
Tracing embeddings model...
Converting embeddings model with input shape: EnumeratedShapes([(1, 1), (1, 64)], default=[1, 1])
Saving model to llama_embeddings.mlpackage

Model verification:
Input names: Features(input_ids)
Output names: Features(hidden_states)
Step 2: Converting LM Head

Converting model from: /Users/felixlin/models/llama-3-8b-Instruct
Output filename prefix: llama
Batch size: 64
Context length: 4096
LUT quantization: 6 bits
Converting part(s): 3
Loading config from /Users/felixlin/models/llama-3-8b-Instruct/config.json

Loaded model config:
  hidden_size: 4096
  vocab_size: 128256
Created lm_head8_1 through lm_head8_8

Loading pretrained weights...
Loading pretrained weights...
Split lm_head weight into lm_head8_1.weight with shape torch.Size([16032, 4096, 1, 1])
Split lm_head weight into lm_head8_2.weight with shape torch.Size([16032, 4096, 1, 1])
Split lm_head weight into lm_head8_3.weight with shape torch.Size([16032, 4096, 1, 1])
Split lm_head weight into lm_head8_4.weight with shape torch.Size([16032, 4096, 1, 1])
Split lm_head weight into lm_head8_5.weight with shape torch.Size([16032, 4096, 1, 1])
Split lm_head weight into lm_head8_6.weight with shape torch.Size([16032, 4096, 1, 1])
Split lm_head weight into lm_head8_7.weight with shape torch.Size([16032, 4096, 1, 1])
Split lm_head weight into lm_head8_8.weight with shape torch.Size([16032, 4096, 1, 1])
Loading model.embed_tokens.weight with shape torch.Size([128256, 4096])
Moving model.embed_tokens.weight to embed_tokens.weight
Missing keys: ['model.kv_cache_0', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.input_layernorm.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.input_layernorm.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.input_layernorm.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.input_layernorm.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.norm.weight']
Reshaped layers.31.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.0.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.0.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.0.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.0.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.0.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.0.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.0.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.1.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.1.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.1.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.1.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.1.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.1.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.1.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.2.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.2.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.2.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.2.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.2.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.2.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.2.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.3.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.3.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.3.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.3.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.3.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.3.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.3.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.4.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.4.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.4.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.4.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.4.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.4.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.4.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.5.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.5.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.5.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.5.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.5.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.5.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.5.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.6.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.6.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.6.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.6.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.6.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.6.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.6.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.7.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.7.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.7.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.7.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.7.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.7.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.7.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.8.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.8.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.8.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.8.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.8.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.8.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.8.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.20.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.20.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.21.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.21.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.21.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.21.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.21.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.21.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.21.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.22.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.22.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.22.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.22.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.22.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.22.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.22.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.23.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.23.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.23.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.23.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.23.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.23.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.23.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.24.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.24.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.24.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.24.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.24.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.24.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.24.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.25.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.25.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.25.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.25.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.25.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.25.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.25.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.26.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.26.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.26.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.26.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.26.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.26.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.26.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.27.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.27.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.27.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.27.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.27.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.27.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.27.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.28.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.28.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.28.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.28.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.28.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.28.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.28.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.29.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.29.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.29.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.29.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.29.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.29.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.29.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.30.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.30.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.30.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.30.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.30.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.30.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.30.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.31.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.31.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.31.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.31.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.31.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.31.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.10.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.10.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.10.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.10.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.10.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.10.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.10.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.11.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.11.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.11.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.11.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.11.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.11.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.11.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.12.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.12.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.12.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.12.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.12.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.12.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.12.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.13.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.13.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.13.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.13.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.13.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.13.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.13.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.14.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.14.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.14.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.14.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.14.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.14.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.14.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.15.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.15.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.15.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.15.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.15.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.15.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.15.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.16.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.16.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.16.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.16.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.16.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.16.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.16.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.17.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.17.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.17.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.17.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.17.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.17.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.17.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.18.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.18.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.18.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.18.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.18.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.18.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.18.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.19.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.19.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.19.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.19.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.19.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.19.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.19.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.20.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.20.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.20.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.20.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.20.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.9.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.9.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.9.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.9.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.9.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.9.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.9.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Pretrained weights loaded successfully
Note: The following expected buffers were initialized:
  - kv_cache_0
  - layers.0.self_attn.rotary_emb.inv_freq
  - layers.1.self_attn.rotary_emb.inv_freq
  - layers.2.self_attn.rotary_emb.inv_freq
  - layers.3.self_attn.rotary_emb.inv_freq
  - layers.4.self_attn.rotary_emb.inv_freq
  - layers.5.self_attn.rotary_emb.inv_freq
  - layers.6.self_attn.rotary_emb.inv_freq
  - layers.7.self_attn.rotary_emb.inv_freq
  - layers.8.self_attn.rotary_emb.inv_freq
  - layers.9.self_attn.rotary_emb.inv_freq
  - layers.10.self_attn.rotary_emb.inv_freq
  - layers.11.self_attn.rotary_emb.inv_freq
  - layers.12.self_attn.rotary_emb.inv_freq
  - layers.13.self_attn.rotary_emb.inv_freq
  - layers.14.self_attn.rotary_emb.inv_freq
  - layers.15.self_attn.rotary_emb.inv_freq
  - layers.16.self_attn.rotary_emb.inv_freq
  - layers.17.self_attn.rotary_emb.inv_freq
  - layers.18.self_attn.rotary_emb.inv_freq
  - layers.19.self_attn.rotary_emb.inv_freq
  - layers.20.self_attn.rotary_emb.inv_freq
  - layers.21.self_attn.rotary_emb.inv_freq
  - layers.22.self_attn.rotary_emb.inv_freq
  - layers.23.self_attn.rotary_emb.inv_freq
  - layers.24.self_attn.rotary_emb.inv_freq
  - layers.25.self_attn.rotary_emb.inv_freq
  - layers.26.self_attn.rotary_emb.inv_freq
  - layers.27.self_attn.rotary_emb.inv_freq
  - layers.28.self_attn.rotary_emb.inv_freq
  - layers.29.self_attn.rotary_emb.inv_freq
  - layers.30.self_attn.rotary_emb.inv_freq
  - layers.31.self_attn.rotary_emb.inv_freq

Converting model part: 3 output_path: llama_lm_head_lut6.mlpackage
Preparing model for conversion...
Moving model to device: cpu
Freezing model parameters...
Model preprocessing completed

Converting LM head layer...
Tracing LM head model...
Applying LUT quantization with 6 bits...
LUT quantization completed
Saving model to llama_lm_head_lut6.mlpackage

Model verification:
Input names: Features(hidden_states)
Output names: Features(logits1,logits2,logits3,logits4,logits5,logits6,logits7,logits8)
Step 3: Converting FFN

Converting model from: /Users/felixlin/models/llama-3-8b-Instruct
Output filename prefix: llama
Batch size: 64
Context length: 4096
LUT quantization: 4 bits
Splitting into 2 chunks
Converting part(s): 2
Loading config from /Users/felixlin/models/llama-3-8b-Instruct/config.json

Loaded model config:
  hidden_size: 4096
  vocab_size: 128256
Created lm_head8_1 through lm_head8_8

Loading pretrained weights...
Loading pretrained weights...
Split lm_head weight into lm_head8_1.weight with shape torch.Size([16032, 4096, 1, 1])
Split lm_head weight into lm_head8_2.weight with shape torch.Size([16032, 4096, 1, 1])
Split lm_head weight into lm_head8_3.weight with shape torch.Size([16032, 4096, 1, 1])
Split lm_head weight into lm_head8_4.weight with shape torch.Size([16032, 4096, 1, 1])
Split lm_head weight into lm_head8_5.weight with shape torch.Size([16032, 4096, 1, 1])
Split lm_head weight into lm_head8_6.weight with shape torch.Size([16032, 4096, 1, 1])
Split lm_head weight into lm_head8_7.weight with shape torch.Size([16032, 4096, 1, 1])
Split lm_head weight into lm_head8_8.weight with shape torch.Size([16032, 4096, 1, 1])
Loading model.embed_tokens.weight with shape torch.Size([128256, 4096])
Moving model.embed_tokens.weight to embed_tokens.weight
Missing keys: ['model.kv_cache_0', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.input_layernorm.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.input_layernorm.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.input_layernorm.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.input_layernorm.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.norm.weight']
Reshaped layers.31.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.0.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.0.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.0.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.0.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.0.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.0.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.0.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.1.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.1.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.1.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.1.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.1.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.1.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.1.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.2.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.2.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.2.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.2.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.2.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.2.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.2.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.3.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.3.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.3.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.3.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.3.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.3.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.3.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.4.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.4.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.4.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.4.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.4.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.4.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.4.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.5.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.5.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.5.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.5.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.5.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.5.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.5.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.6.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.6.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.6.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.6.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.6.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.6.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.6.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.7.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.7.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.7.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.7.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.7.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.7.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.7.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.8.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.8.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.8.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.8.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.8.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.8.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.8.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.20.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.20.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.21.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.21.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.21.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.21.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.21.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.21.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.21.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.22.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.22.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.22.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.22.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.22.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.22.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.22.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.23.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.23.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.23.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.23.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.23.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.23.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.23.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.24.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.24.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.24.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.24.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.24.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.24.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.24.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.25.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.25.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.25.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.25.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.25.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.25.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.25.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.26.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.26.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.26.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.26.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.26.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.26.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.26.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.27.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.27.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.27.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.27.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.27.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.27.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.27.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.28.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.28.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.28.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.28.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.28.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.28.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.28.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.29.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.29.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.29.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.29.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.29.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.29.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.29.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.30.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.30.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.30.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.30.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.30.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.30.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.30.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.31.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.31.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.31.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.31.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.31.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.31.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.10.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.10.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.10.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.10.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.10.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.10.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.10.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.11.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.11.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.11.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.11.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.11.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.11.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.11.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.12.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.12.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.12.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.12.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.12.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.12.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.12.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.13.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.13.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.13.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.13.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.13.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.13.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.13.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.14.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.14.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.14.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.14.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.14.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.14.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.14.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.15.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.15.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.15.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.15.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.15.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.15.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.15.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.16.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.16.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.16.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.16.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.16.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.16.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.16.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.17.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.17.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.17.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.17.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.17.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.17.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.17.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.18.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.18.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.18.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.18.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.18.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.18.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.18.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.19.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.19.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.19.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.19.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.19.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.19.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.19.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.20.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.20.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.20.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.20.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.20.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.9.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.9.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.9.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.9.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.9.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.9.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.9.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Pretrained weights loaded successfully
Note: The following expected buffers were initialized:
  - kv_cache_0
  - layers.0.self_attn.rotary_emb.inv_freq
  - layers.1.self_attn.rotary_emb.inv_freq
  - layers.2.self_attn.rotary_emb.inv_freq
  - layers.3.self_attn.rotary_emb.inv_freq
  - layers.4.self_attn.rotary_emb.inv_freq
  - layers.5.self_attn.rotary_emb.inv_freq
  - layers.6.self_attn.rotary_emb.inv_freq
  - layers.7.self_attn.rotary_emb.inv_freq
  - layers.8.self_attn.rotary_emb.inv_freq
  - layers.9.self_attn.rotary_emb.inv_freq
  - layers.10.self_attn.rotary_emb.inv_freq
  - layers.11.self_attn.rotary_emb.inv_freq
  - layers.12.self_attn.rotary_emb.inv_freq
  - layers.13.self_attn.rotary_emb.inv_freq
  - layers.14.self_attn.rotary_emb.inv_freq
  - layers.15.self_attn.rotary_emb.inv_freq
  - layers.16.self_attn.rotary_emb.inv_freq
  - layers.17.self_attn.rotary_emb.inv_freq
  - layers.18.self_attn.rotary_emb.inv_freq
  - layers.19.self_attn.rotary_emb.inv_freq
  - layers.20.self_attn.rotary_emb.inv_freq
  - layers.21.self_attn.rotary_emb.inv_freq
  - layers.22.self_attn.rotary_emb.inv_freq
  - layers.23.self_attn.rotary_emb.inv_freq
  - layers.24.self_attn.rotary_emb.inv_freq
  - layers.25.self_attn.rotary_emb.inv_freq
  - layers.26.self_attn.rotary_emb.inv_freq
  - layers.27.self_attn.rotary_emb.inv_freq
  - layers.28.self_attn.rotary_emb.inv_freq
  - layers.29.self_attn.rotary_emb.inv_freq
  - layers.30.self_attn.rotary_emb.inv_freq
  - layers.31.self_attn.rotary_emb.inv_freq

Converting chunk 1/2

Converting FFN layers...
Processing chunk 1/2
  Total layers: 32
  Layers per chunk: 16
  This chunk: layers [0..15]
  First chunk: includes input layer
GetTransformerStates part=2 ENABLE_UNIFIED_CACHE=True num_layers_this_part=64 model.config.num_hidden_layers=32
Tracing FFN model...
FFN layers conversion completed
Applying LUT quantization with 4 bits and 8 channels per group using 1 worker(s)...
LUT quantization completed
Saving chunk to llama_FFN_lut4_chunk_01of02.mlpackage

Converting chunk 2/2

Converting FFN layers...
Processing chunk 2/2
  Total layers: 32
  Layers per chunk: 16
  This chunk: layers [16..31]
  Last chunk: includes output layer
GetTransformerStates part=2 ENABLE_UNIFIED_CACHE=True num_layers_this_part=64 model.config.num_hidden_layers=32
Tracing FFN model...
FFN layers conversion completed
Applying LUT quantization with 4 bits and 8 channels per group using 1 worker(s)...
LUT quantization completed
Saving chunk to llama_FFN_lut4_chunk_02of02.mlpackage

Model verification:

Chunk 1:
Input names: Features(hidden_states,position_ids,causal_mask,current_pos)
Output names: Features(output_hidden_states)

Chunk 2:
Input names: Features(hidden_states,position_ids,causal_mask,current_pos)
Output names: Features(output_hidden_states)
Step 4: Converting Prefill

Converting model from: /Users/felixlin/models/llama-3-8b-Instruct
Output filename prefix: llama
Batch size: 64
Context length: 4096
LUT quantization: 4 bits
Splitting into 2 chunks
Converting part(s): 2_prefill
Loading config from /Users/felixlin/models/llama-3-8b-Instruct/config.json

Loaded model config:
  hidden_size: 4096
  vocab_size: 128256
Created lm_head8_1 through lm_head8_8

Loading pretrained weights...
Loading pretrained weights...
Split lm_head weight into lm_head8_1.weight with shape torch.Size([16032, 4096, 1, 1])
Split lm_head weight into lm_head8_2.weight with shape torch.Size([16032, 4096, 1, 1])
Split lm_head weight into lm_head8_3.weight with shape torch.Size([16032, 4096, 1, 1])
Split lm_head weight into lm_head8_4.weight with shape torch.Size([16032, 4096, 1, 1])
Split lm_head weight into lm_head8_5.weight with shape torch.Size([16032, 4096, 1, 1])
Split lm_head weight into lm_head8_6.weight with shape torch.Size([16032, 4096, 1, 1])
Split lm_head weight into lm_head8_7.weight with shape torch.Size([16032, 4096, 1, 1])
Split lm_head weight into lm_head8_8.weight with shape torch.Size([16032, 4096, 1, 1])
Loading model.embed_tokens.weight with shape torch.Size([128256, 4096])
Moving model.embed_tokens.weight to embed_tokens.weight
Missing keys: ['model.kv_cache_0', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.input_layernorm.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.input_layernorm.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.input_layernorm.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.input_layernorm.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.norm.weight']
Reshaped layers.31.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.0.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.0.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.0.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.0.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.0.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.0.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.0.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.1.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.1.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.1.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.1.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.1.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.1.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.1.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.2.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.2.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.2.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.2.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.2.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.2.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.2.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.3.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.3.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.3.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.3.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.3.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.3.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.3.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.4.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.4.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.4.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.4.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.4.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.4.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.4.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.5.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.5.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.5.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.5.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.5.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.5.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.5.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.6.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.6.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.6.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.6.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.6.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.6.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.6.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.7.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.7.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.7.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.7.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.7.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.7.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.7.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.8.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.8.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.8.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.8.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.8.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.8.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.8.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.20.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.20.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.21.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.21.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.21.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.21.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.21.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.21.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.21.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.22.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.22.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.22.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.22.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.22.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.22.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.22.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.23.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.23.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.23.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.23.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.23.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.23.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.23.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.24.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.24.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.24.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.24.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.24.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.24.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.24.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.25.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.25.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.25.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.25.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.25.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.25.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.25.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.26.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.26.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.26.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.26.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.26.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.26.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.26.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.27.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.27.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.27.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.27.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.27.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.27.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.27.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.28.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.28.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.28.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.28.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.28.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.28.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.28.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.29.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.29.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.29.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.29.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.29.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.29.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.29.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.30.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.30.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.30.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.30.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.30.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.30.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.30.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.31.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.31.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.31.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.31.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.31.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.31.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.10.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.10.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.10.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.10.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.10.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.10.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.10.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.11.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.11.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.11.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.11.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.11.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.11.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.11.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.12.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.12.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.12.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.12.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.12.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.12.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.12.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.13.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.13.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.13.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.13.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.13.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.13.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.13.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.14.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.14.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.14.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.14.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.14.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.14.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.14.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.15.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.15.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.15.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.15.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.15.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.15.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.15.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.16.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.16.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.16.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.16.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.16.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.16.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.16.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.17.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.17.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.17.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.17.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.17.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.17.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.17.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.18.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.18.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.18.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.18.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.18.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.18.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.18.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.19.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.19.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.19.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.19.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.19.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.19.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.19.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.20.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.20.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.20.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.20.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.20.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.9.mlp.down_proj.weight from torch.Size([4096, 14336]) to torch.Size([4096, 14336, 1, 1])
Reshaped layers.9.mlp.gate_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.9.mlp.up_proj.weight from torch.Size([14336, 4096]) to torch.Size([14336, 4096, 1, 1])
Reshaped layers.9.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.9.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.9.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.9.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Pretrained weights loaded successfully
Note: The following expected buffers were initialized:
  - kv_cache_0
  - layers.0.self_attn.rotary_emb.inv_freq
  - layers.1.self_attn.rotary_emb.inv_freq
  - layers.2.self_attn.rotary_emb.inv_freq
  - layers.3.self_attn.rotary_emb.inv_freq
  - layers.4.self_attn.rotary_emb.inv_freq
  - layers.5.self_attn.rotary_emb.inv_freq
  - layers.6.self_attn.rotary_emb.inv_freq
  - layers.7.self_attn.rotary_emb.inv_freq
  - layers.8.self_attn.rotary_emb.inv_freq
  - layers.9.self_attn.rotary_emb.inv_freq
  - layers.10.self_attn.rotary_emb.inv_freq
  - layers.11.self_attn.rotary_emb.inv_freq
  - layers.12.self_attn.rotary_emb.inv_freq
  - layers.13.self_attn.rotary_emb.inv_freq
  - layers.14.self_attn.rotary_emb.inv_freq
  - layers.15.self_attn.rotary_emb.inv_freq
  - layers.16.self_attn.rotary_emb.inv_freq
  - layers.17.self_attn.rotary_emb.inv_freq
  - layers.18.self_attn.rotary_emb.inv_freq
  - layers.19.self_attn.rotary_emb.inv_freq
  - layers.20.self_attn.rotary_emb.inv_freq
  - layers.21.self_attn.rotary_emb.inv_freq
  - layers.22.self_attn.rotary_emb.inv_freq
  - layers.23.self_attn.rotary_emb.inv_freq
  - layers.24.self_attn.rotary_emb.inv_freq
  - layers.25.self_attn.rotary_emb.inv_freq
  - layers.26.self_attn.rotary_emb.inv_freq
  - layers.27.self_attn.rotary_emb.inv_freq
  - layers.28.self_attn.rotary_emb.inv_freq
  - layers.29.self_attn.rotary_emb.inv_freq
  - layers.30.self_attn.rotary_emb.inv_freq
  - layers.31.self_attn.rotary_emb.inv_freq

Converting chunk 1/2

Converting transformer prefill mode...
Processing chunk 1/2 (layers 0 to 15)
GetTransformerStates part=2_prefill ENABLE_UNIFIED_CACHE=True num_layers_this_part=64 model.config.num_hidden_layers=32
Tracing prefill model...
Prefill mode conversion completed
Applying LUT quantization with 4 bits and 8 channels per group using 1 worker(s)...
LUT quantization completed
Saving chunk to llama_prefill_lut4_chunk_01of02.mlpackage

Converting chunk 2/2

Converting transformer prefill mode...
Processing chunk 2/2 (layers 16 to 31)
GetTransformerStates part=2_prefill ENABLE_UNIFIED_CACHE=True num_layers_this_part=64 model.config.num_hidden_layers=32
Tracing prefill model...
Skipping MLP for last layer in prefill mode
Skipping final normalization for prefill, data not used!
Skipping MLP for last layer in prefill mode
Skipping final normalization for prefill, data not used!
Skipping MLP for last layer in prefill mode
Skipping final normalization for prefill, data not used!
Prefill mode conversion completed
Applying LUT quantization with 4 bits and 8 channels per group using 1 worker(s)...
LUT quantization completed
Saving chunk to llama_prefill_lut4_chunk_02of02.mlpackage

Model verification:

Chunk 1:
Input names: Features(hidden_states,position_ids,causal_mask,current_pos)
Output names: Features(output_hidden_states)

Chunk 2:
Input names: Features(hidden_states,position_ids,causal_mask,current_pos)
Output names: Features(output_hidden_states)
Step 5: Combining Models

Debug: Validating chunk files:
  Current dir: /Users/felixlin/models/llama-3-8b-Instruct-ctx4096-coreml-fxl
  Num chunks: 2
  LUT bits: 4
  Prefix: llama

Looking for files with patterns:
  FFN: llama_FFN_lut4_chunk_{:02d}of02.mlpackage
  PF:  llama_prefill_lut4_chunk_{:02d}of02.mlpackage

Files in directory:
  llama_embeddings.mlpackage
  llama_prefill_lut4_chunk_02of02.mlpackage
  llama_prefill_lut4_chunk_01of02.mlpackage
  llama_lm_head_lut6.mlpackage
  llama_FFN_lut4_chunk_02of02.mlpackage
  llama_FFN_lut4_chunk_01of02.mlpackage

Processing chunk 1:
  FFN: llama_FFN_lut4_chunk_01of02.mlpackage
  Prefill: llama_prefill_lut4_chunk_01of02.mlpackage
  Output: llama_FFN_PF_lut4_chunk_01of02.mlpackage
Creating combined model...
Loading combined model...
Adding metadata...

DEBUG: Starting AddCombinedMetadata...
DEBUG: Processing source models...
DEBUG: Reading metadata from model 1
DEBUG: Got metadata: {'author': 'Converted with Anemll v0.3.0', 'version': '0.3.0', 'short_description': 'Anemll Model (FFN) converted to CoreML', 'com.github.apple.coremltools.version': '8.2', 'lut_bits': '4', 'com.github.apple.coremltools.source_dialect': 'TorchScript', 'info': 'Converted with Anemll v0.3.0', 'chunk_no': '1', 'com.github.apple.coremltools.source': 'torch==2.5.0', 'num_chunks': '2', 'context_length': '4096'}
DEBUG: Reading metadata from model 2
DEBUG: Got metadata: {'author': 'Converted with Anemll v0.3.0', 'version': '0.3.0', 'short_description': 'Anemll Model (Prefill) converted to CoreML', 'com.github.apple.coremltools.version': '8.2', 'lut_bits': '4', 'com.github.apple.coremltools.source_dialect': 'TorchScript', 'info': 'Converted with Anemll v0.3.0', 'chunk_no': '1', 'com.github.apple.coremltools.source': 'torch==2.5.0', 'num_chunks': '2', 'batch_size': '64', 'context_length': '4096'}

DEBUG: Combined metadata: {'author': 'Converted with Anemll v0.3.0', 'version': '0.3.0', 'com.github.apple.coremltools.version': '8.2', 'lut_bits': '4', 'com.github.apple.coremltools.source_dialect': 'TorchScript', 'info': 'Converted with Anemll v0.3.0', 'chunk_no': '1', 'com.github.apple.coremltools.source': 'torch==2.5.0', 'num_chunks': '2', 'context_length': '4096', 'batch_size': '64', 'short_description': 'Anemll Model: Multifunction FFN+Prefill'}
DEBUG: Adding metadata to target model...
DEBUG: Setting description to: Anemll Model: Multifunction FFN+Prefill
DEBUG: Metadata added successfully
Saving final model to: llama_FFN_PF_lut4_chunk_01of02.mlpackage
Successfully combined chunk 1

Processing chunk 2:
  FFN: llama_FFN_lut4_chunk_02of02.mlpackage
  Prefill: llama_prefill_lut4_chunk_02of02.mlpackage
  Output: llama_FFN_PF_lut4_chunk_02of02.mlpackage
Creating combined model...
Loading combined model...
Adding metadata...

DEBUG: Starting AddCombinedMetadata...
DEBUG: Processing source models...
DEBUG: Reading metadata from model 1
DEBUG: Got metadata: {'author': 'Converted with Anemll v0.3.0', 'version': '0.3.0', 'short_description': 'Anemll Model (FFN) converted to CoreML', 'com.github.apple.coremltools.version': '8.2', 'lut_bits': '4', 'com.github.apple.coremltools.source_dialect': 'TorchScript', 'info': 'Converted with Anemll v0.3.0', 'chunk_no': '2', 'com.github.apple.coremltools.source': 'torch==2.5.0', 'num_chunks': '2', 'context_length': '4096'}
DEBUG: Reading metadata from model 2
DEBUG: Got metadata: {'author': 'Converted with Anemll v0.3.0', 'version': '0.3.0', 'short_description': 'Anemll Model (Prefill) converted to CoreML', 'com.github.apple.coremltools.version': '8.2', 'lut_bits': '4', 'com.github.apple.coremltools.source_dialect': 'TorchScript', 'info': 'Converted with Anemll v0.3.0', 'chunk_no': '2', 'com.github.apple.coremltools.source': 'torch==2.5.0', 'num_chunks': '2', 'batch_size': '64', 'context_length': '4096'}

DEBUG: Combined metadata: {'author': 'Converted with Anemll v0.3.0', 'version': '0.3.0', 'com.github.apple.coremltools.version': '8.2', 'lut_bits': '4', 'com.github.apple.coremltools.source_dialect': 'TorchScript', 'info': 'Converted with Anemll v0.3.0', 'chunk_no': '2', 'com.github.apple.coremltools.source': 'torch==2.5.0', 'num_chunks': '2', 'context_length': '4096', 'batch_size': '64', 'short_description': 'Anemll Model: Multifunction FFN+Prefill'}
DEBUG: Adding metadata to target model...
DEBUG: Setting description to: Anemll Model: Multifunction FFN+Prefill
DEBUG: Metadata added successfully
Saving final model to: llama_FFN_PF_lut4_chunk_02of02.mlpackage
Successfully combined chunk 2

All chunks combined successfully!
Step 6: Compiling Models Part 1

Compiling llama_embeddings.mlpackage...
Compilation successful
Step 6: Compiling Models Part 3

Compiling llama_lm_head_lut6.mlpackage...
Compilation successful
Step 6: Compiling Models Part 2

Compiling llama_FFN_PF_lut4_chunk_01of02.mlpackage...
Compilation successful

Compiling llama_FFN_PF_lut4_chunk_02of02.mlpackage...
Compilation successful
Step 7: Copying tokenizer files and creating meta.yaml
Creating config.json for iOS tokenizer...
✅ Created config.json at: /Users/felixlin/models/llama-3-8b-Instruct-ctx4096-coreml-fxl/config.json
Step 8: Testing with chat.py

Loaded parameters from /Users/felixlin/models/llama-3-8b-Instruct-ctx4096-coreml-fxl/meta.yaml:
  Context Length: 4096
  Batch Size: 64
  Num Chunks: 2
  Models Directory: /Users/felixlin/models/llama-3-8b-Instruct-ctx4096-coreml-fxl
  Embeddings: llama_embeddings
  LM Head: llama_lm_head_lut6
  FFN: llama_FFN_PF_lut4_chunk_01of02

Using model directory: /Users/felixlin/models/llama-3-8b-Instruct-ctx4096-coreml-fxl
Context length: 4096
Using tokenizer path: /Users/felixlin/models/llama-3-8b-Instruct-ctx4096-coreml-fxl

Loading models...

Loading embeddings model...
Found model at: /Users/felixlin/models/llama-3-8b-Instruct-ctx4096-coreml-fxl/llama_embeddings.mlmodelc
Loading from: /Users/felixlin/models/llama-3-8b-Instruct-ctx4096-coreml-fxl/llama_embeddings.mlmodelc
Embeddings model loaded successfully

Warning: No metadata found in model

Using parameters:
  Context Length: 4096
  State Length: 4096
  Prefill Batch Size: 64
  LUT Bits: 4
  Number of Chunks: 2

Overriding batch size from args: 64

Overriding num chunks from args: 2

Loading LM head model...
Found model at: /Users/felixlin/models/llama-3-8b-Instruct-ctx4096-coreml-fxl/llama_lm_head_lut6.mlmodelc
Loading from: /Users/felixlin/models/llama-3-8b-Instruct-ctx4096-coreml-fxl/llama_lm_head_lut6.mlmodelc
LM head model loaded successfully

Loading FFN+PREFILL model(s)...
Found model at: /Users/felixlin/models/llama-3-8b-Instruct-ctx4096-coreml-fxl/llama_FFN_PF_lut4_chunk_01of02.mlmodelc

Detected chunked FFN+PREFILL model (2 chunks)

Loading FFN+PREFILL chunk: llama_FFN_PF_lut4_chunk_01of02.mlmodelc
Chunk loaded successfully

Loading FFN+PREFILL chunk: llama_FFN_PF_lut4_chunk_02of02.mlmodelc
Chunk loaded successfully

Warning: No metadata found in model

Using parameters:
  Context Length: 4096
  State Length: 4096
  Prefill Batch Size: 64
  LUT Bits: 4
  Number of Chunks: 2

Overriding batch size from args: 64

Overriding num chunks from args: 2

Metadata befor args.context_length: {'context_length': 4096, 'state_length': 4096, 'batch_size': 64, 'lut_bits': 4, 'num_chunks': 2}

Overriding context length from command line: 4096

Metadata after load_models: {'context_length': 4096, 'state_length': 4096, 'batch_size': 64, 'lut_bits': 4, 'num_chunks': 2}

Tokenizer Configuration:
Tokenizer type: <class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>
Tokenizer name: PreTrainedTokenizerFast
Vocabulary size: 128256
Model max length: 1000000000000000019884624838656

Special Tokens:
PAD token: '<|reserved_special_token_250|>' (ID: 128255)
EOS token: '<|eot_id|>' (ID: 128009)
BOS token: '<|begin_of_text|>' (ID: 128000)
UNK token: 'None' (ID: None)

Created unified transformer state for 2 chunks

Initialized causal mask for context length 4096
[0m
[0m

Using context length: 4096

Starting chat session. Press Ctrl+D to exit.
Type your message and press Enter to chat.

Using chat template for prompts

[92mYou:[0m Who are you ?

[94mAssistant:[0m I am LLaMA, an AI model developed by Meta AI that can understand and respond to human input in the form of text. I'm a large language model, trained on a massive dataset of text from the internet, which allows me to generate human-like responses to a wide range of topics and questions. I'm not a human, but a computer program designed to simulate conversation and answer questions to the best of my ability based on my training. I don't have personal opinions, emotions, or experiences, but I'm here to help you with any questions or topics you'd like to discuss!
[34m6.3 t/s[0m

Prefill: 271.8ms (51.5 t/s)
Inference: 6.4 t/s
Total: Generated 121 tokens in 19.24s

To chat with the model, use:

Option 1 - Using meta.yaml (recommended):
python /Users/felixlin/workspace-apple-silicon/Anemll/tests/chat.py \
    --meta "/Users/felixlin/models/llama-3-8b-Instruct-ctx4096-coreml-fxl/meta.yaml"

Or for full conversation mode:
python /Users/felixlin/workspace-apple-silicon/Anemll/tests/chat_full.py \
    --meta "/Users/felixlin/models/llama-3-8b-Instruct-ctx4096-coreml-fxl/meta.yaml"

Option 2 - Manual configuration:
python /Users/felixlin/workspace-apple-silicon/Anemll/tests/chat.py \
    --embed llama_embeddings \
    --lmhead llama_lm_head_lut6 \
    --ffn llama_FFN_PF_lut4_chunk_01of02 \
    --tokenizer "/Users/felixlin/models/llama-3-8b-Instruct-ctx4096-coreml-fxl" \
    --context-length 4096 \
    --d "/Users/felixlin/models/llama-3-8b-Instruct-ctx4096-coreml-fxl"
Conversion completed successfully!
