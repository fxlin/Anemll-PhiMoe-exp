Using imported TEST_DEVICE: cpu

setUpClass is being called...
Using device: cpu
Loading config from /Users/felixlin/models/Phi-3.5-MoE-instruct/config.json
Loading config from /Users/felixlin/models/Phi-3.5-MoE-instruct/config.json
Loaded model config:
  hidden_size: 4096
  intermediate_size: 6400
  num_attention_heads: 32
  num_hidden_layers: 2
  num_key_value_heads: 8
  vocab_size: 32064
  max_position_embeddings: 131072
  rope_theta: 10000.0

[TRACE] RotaryEmbedding initialized:
  dim: 128
  max_position_embeddings: 131072
  base: 10000.0
  inv_freq shape: torch.Size([64])
  cos_cached shape: torch.Size([1, 131072, 128])
  sin_cached shape: torch.Size([1, 131072, 128])
  cos_cached[0,0,:5]: [1.2431631088256836, 1.2431631088256836, 1.2431631088256836, 1.2431631088256836, 1.2431631088256836]
  sin_cached[0,0,:5]: [0.0, 0.0, 0.0, 0.0, 0.0]

[TRACE] RotaryEmbedding initialized:
  dim: 128
  max_position_embeddings: 131072
  base: 10000.0
  inv_freq shape: torch.Size([64])
  cos_cached shape: torch.Size([1, 131072, 128])
  sin_cached shape: torch.Size([1, 131072, 128])
  cos_cached[0,0,:5]: [1.2431631088256836, 1.2431631088256836, 1.2431631088256836, 1.2431631088256836, 1.2431631088256836]
  sin_cached[0,0,:5]: [0.0, 0.0, 0.0, 0.0, 0.0]
Initialized unified KV kv_cache_0 with shape: torch.Size([4, 8, 512, 128])
Created lm_head8_1 through lm_head8_8
Loading pretrained weights from /Users/felixlin/models/Phi-3.5-MoE-instruct
Loading pretrained weights...
Split lm_head weight into lm_head8_1.weight with shape torch.Size([4008, 4096, 1, 1])
Split lm_head weight into lm_head8_2.weight with shape torch.Size([4008, 4096, 1, 1])
Split lm_head weight into lm_head8_3.weight with shape torch.Size([4008, 4096, 1, 1])
Split lm_head weight into lm_head8_4.weight with shape torch.Size([4008, 4096, 1, 1])
Split lm_head weight into lm_head8_5.weight with shape torch.Size([4008, 4096, 1, 1])
Split lm_head weight into lm_head8_6.weight with shape torch.Size([4008, 4096, 1, 1])
Split lm_head weight into lm_head8_7.weight with shape torch.Size([4008, 4096, 1, 1])
Split lm_head weight into lm_head8_8.weight with shape torch.Size([4008, 4096, 1, 1])
Loading model.embed_tokens.weight with shape torch.Size([32064, 4096])
Moving model.embed_tokens.weight to embed_tokens.weight
(after loading emb/head) Missing keys: ['model.kv_cache_0', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.0.block_sparse_moe.gate.weight', 'model.layers.0.block_sparse_moe.experts.0.w1.weight', 'model.layers.0.block_sparse_moe.experts.0.w2.weight', 'model.layers.0.block_sparse_moe.experts.0.w3.weight', 'model.layers.0.block_sparse_moe.experts.1.w1.weight', 'model.layers.0.block_sparse_moe.experts.1.w2.weight', 'model.layers.0.block_sparse_moe.experts.1.w3.weight', 'model.layers.0.block_sparse_moe.experts.2.w1.weight', 'model.layers.0.block_sparse_moe.experts.2.w2.weight', 'model.layers.0.block_sparse_moe.experts.2.w3.weight', 'model.layers.0.block_sparse_moe.experts.3.w1.weight', 'model.layers.0.block_sparse_moe.experts.3.w2.weight', 'model.layers.0.block_sparse_moe.experts.3.w3.weight', 'model.layers.0.block_sparse_moe.experts.4.w1.weight', 'model.layers.0.block_sparse_moe.experts.4.w2.weight', 'model.layers.0.block_sparse_moe.experts.4.w3.weight', 'model.layers.0.block_sparse_moe.experts.5.w1.weight', 'model.layers.0.block_sparse_moe.experts.5.w2.weight', 'model.layers.0.block_sparse_moe.experts.5.w3.weight', 'model.layers.0.block_sparse_moe.experts.6.w1.weight', 'model.layers.0.block_sparse_moe.experts.6.w2.weight', 'model.layers.0.block_sparse_moe.experts.6.w3.weight', 'model.layers.0.block_sparse_moe.experts.7.w1.weight', 'model.layers.0.block_sparse_moe.experts.7.w2.weight', 'model.layers.0.block_sparse_moe.experts.7.w3.weight', 'model.layers.0.block_sparse_moe.experts.8.w1.weight', 'model.layers.0.block_sparse_moe.experts.8.w2.weight', 'model.layers.0.block_sparse_moe.experts.8.w3.weight', 'model.layers.0.block_sparse_moe.experts.9.w1.weight', 'model.layers.0.block_sparse_moe.experts.9.w2.weight', 'model.layers.0.block_sparse_moe.experts.9.w3.weight', 'model.layers.0.block_sparse_moe.experts.10.w1.weight', 'model.layers.0.block_sparse_moe.experts.10.w2.weight', 'model.layers.0.block_sparse_moe.experts.10.w3.weight', 'model.layers.0.block_sparse_moe.experts.11.w1.weight', 'model.layers.0.block_sparse_moe.experts.11.w2.weight', 'model.layers.0.block_sparse_moe.experts.11.w3.weight', 'model.layers.0.block_sparse_moe.experts.12.w1.weight', 'model.layers.0.block_sparse_moe.experts.12.w2.weight', 'model.layers.0.block_sparse_moe.experts.12.w3.weight', 'model.layers.0.block_sparse_moe.experts.13.w1.weight', 'model.layers.0.block_sparse_moe.experts.13.w2.weight', 'model.layers.0.block_sparse_moe.experts.13.w3.weight', 'model.layers.0.block_sparse_moe.experts.14.w1.weight', 'model.layers.0.block_sparse_moe.experts.14.w2.weight', 'model.layers.0.block_sparse_moe.experts.14.w3.weight', 'model.layers.0.block_sparse_moe.experts.15.w1.weight', 'model.layers.0.block_sparse_moe.experts.15.w2.weight', 'model.layers.0.block_sparse_moe.experts.15.w3.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.0.input_layernorm.bias', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.0.post_attention_layernorm.bias', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.1.block_sparse_moe.gate.weight', 'model.layers.1.block_sparse_moe.experts.0.w1.weight', 'model.layers.1.block_sparse_moe.experts.0.w2.weight', 'model.layers.1.block_sparse_moe.experts.0.w3.weight', 'model.layers.1.block_sparse_moe.experts.1.w1.weight', 'model.layers.1.block_sparse_moe.experts.1.w2.weight', 'model.layers.1.block_sparse_moe.experts.1.w3.weight', 'model.layers.1.block_sparse_moe.experts.2.w1.weight', 'model.layers.1.block_sparse_moe.experts.2.w2.weight', 'model.layers.1.block_sparse_moe.experts.2.w3.weight', 'model.layers.1.block_sparse_moe.experts.3.w1.weight', 'model.layers.1.block_sparse_moe.experts.3.w2.weight', 'model.layers.1.block_sparse_moe.experts.3.w3.weight', 'model.layers.1.block_sparse_moe.experts.4.w1.weight', 'model.layers.1.block_sparse_moe.experts.4.w2.weight', 'model.layers.1.block_sparse_moe.experts.4.w3.weight', 'model.layers.1.block_sparse_moe.experts.5.w1.weight', 'model.layers.1.block_sparse_moe.experts.5.w2.weight', 'model.layers.1.block_sparse_moe.experts.5.w3.weight', 'model.layers.1.block_sparse_moe.experts.6.w1.weight', 'model.layers.1.block_sparse_moe.experts.6.w2.weight', 'model.layers.1.block_sparse_moe.experts.6.w3.weight', 'model.layers.1.block_sparse_moe.experts.7.w1.weight', 'model.layers.1.block_sparse_moe.experts.7.w2.weight', 'model.layers.1.block_sparse_moe.experts.7.w3.weight', 'model.layers.1.block_sparse_moe.experts.8.w1.weight', 'model.layers.1.block_sparse_moe.experts.8.w2.weight', 'model.layers.1.block_sparse_moe.experts.8.w3.weight', 'model.layers.1.block_sparse_moe.experts.9.w1.weight', 'model.layers.1.block_sparse_moe.experts.9.w2.weight', 'model.layers.1.block_sparse_moe.experts.9.w3.weight', 'model.layers.1.block_sparse_moe.experts.10.w1.weight', 'model.layers.1.block_sparse_moe.experts.10.w2.weight', 'model.layers.1.block_sparse_moe.experts.10.w3.weight', 'model.layers.1.block_sparse_moe.experts.11.w1.weight', 'model.layers.1.block_sparse_moe.experts.11.w2.weight', 'model.layers.1.block_sparse_moe.experts.11.w3.weight', 'model.layers.1.block_sparse_moe.experts.12.w1.weight', 'model.layers.1.block_sparse_moe.experts.12.w2.weight', 'model.layers.1.block_sparse_moe.experts.12.w3.weight', 'model.layers.1.block_sparse_moe.experts.13.w1.weight', 'model.layers.1.block_sparse_moe.experts.13.w2.weight', 'model.layers.1.block_sparse_moe.experts.13.w3.weight', 'model.layers.1.block_sparse_moe.experts.14.w1.weight', 'model.layers.1.block_sparse_moe.experts.14.w2.weight', 'model.layers.1.block_sparse_moe.experts.14.w3.weight', 'model.layers.1.block_sparse_moe.experts.15.w1.weight', 'model.layers.1.block_sparse_moe.experts.15.w2.weight', 'model.layers.1.block_sparse_moe.experts.15.w3.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.1.input_layernorm.bias', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.1.post_attention_layernorm.bias', 'model.norm.weight', 'model.norm.bias']
Reshaped layers.8.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.8.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.8.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.8.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.9.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.9.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.9.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.9.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.21.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.21.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.21.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.22.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.22.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.22.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.22.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.31.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.31.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.31.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.31.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.6.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.6.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.6.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.6.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.7.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.7.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.7.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.7.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.0.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.0.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.0.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.0.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.1.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.1.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.1.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.1.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.23.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.23.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.23.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.23.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.24.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.24.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.24.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.24.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.29.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.29.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.29.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.29.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.30.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.30.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.30.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.30.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.27.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.27.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.27.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.27.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.28.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.28.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.28.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.28.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.18.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.18.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.18.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.18.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.19.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.19.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.19.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.19.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.2.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.2.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.2.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.2.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.3.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.3.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.3.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.3.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.12.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.12.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.12.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.12.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.13.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.13.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.13.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.13.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.14.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.14.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.14.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.14.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.15.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.15.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.15.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.15.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.25.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.25.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.25.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.25.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.26.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.26.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.26.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.26.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.20.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.20.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.20.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.20.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.21.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.16.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.16.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.16.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.16.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.17.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.17.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.17.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.17.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.4.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.4.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.4.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.4.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.5.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.5.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.5.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.5.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.10.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.10.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.10.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.10.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Reshaped layers.11.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Keeping o_proj weights as 2D: layers.11.self_attn.o_proj.weight shape torch.Size([4096, 4096])
Reshaped layers.11.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])
Reshaped layers.11.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])
Pretrained weights loaded successfully
Note: The following expected buffers were initialized:
  - kv_cache_0
  - layers.0.self_attn.rotary_emb.inv_freq
  - layers.1.self_attn.rotary_emb.inv_freq
Tokenizer loaded successfully using AutoTokenizer.
Tokenizer vocabulary size: 32011
Tokenizer default padding side: left
Tokenizer new padding side: left
Tokenizer pad token: <|endoftext|>
Tokenizer pad token ID: 32000

[DEBUG] Tokenizer info:
  EOS token: <|endoftext|>
  EOS token ID: 32000
  Special tokens: {'bos_token': '<s>', 'eos_token': '<|endoftext|>', 'unk_token': '<unk>', 'pad_token': '<|endoftext|>'}

[DEBUG] Template token analysis:
Token sequence:
  0: 29871 -> ''
  1: 32006 -> '<|system|>'
  2: 1688 -> 'test'
  3: 32007 -> '<|end|>'
  Found EOT token ID: 32007
  4: 32000 -> '<|endoftext|>'

[DEBUG] Decoded content:
  Prompt: What is Apple Neural Engine?
  Full decoded input: <|user|> What is Apple Neural Engine? <|end|> <|assistant|>
  Token by token:
    0: 29871 -> ''
    1: 32010 -> '<|user|>'
    2: 5618 -> 'What'
    3: 338 -> 'is'
    4: 12113 -> 'Apple'
    5: 2448 -> 'Ne'
    6: 3631 -> 'ural'
    7: 10863 -> 'Engine'
    8: 29973 -> '?'
    9: 32007 -> '<|end|>'
    10: 32001 -> '<|assistant|>'

[DEBUG] Before context padding:
  Current shape: torch.Size([1, 11])
  Padding to context length: 512
[DEBUG] input_ids length: 512 

Input preparation:
  input_ids shape: torch.Size([1, 512])
  input_ids device: cpu
  input_ids dtype: torch.int64
  prompt_length: 11
  model device: cpu

[DEBUG] Starting prefill phase:
  Total prompt length: 11
  Tokens to process: 10 (leaving last token for prediction)

[DEBUG] Processing batch:
  Current position: 0
  Batch end: 10
  Current batch size: 10
  Padding batch from 10 to 64 tokens
[DEBUG] Position IDs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
cos shape: torch.Size([1, 64, 1, 128])
sin shape: torch.Size([1, 64, 1, 128])
[TRACE] get_rotary_embedding_prefill Batched rotary from pos 0:
  cos shape: torch.Size([1, 64, 1, 128]), values[0,:5]: [1.2431631088256836, 1.2431631088256836, 1.2431631088256836, 1.2431631088256836, 1.2431631088256836]
  sin shape: torch.Size([1, 64, 1, 128]), values[0,:5]: [0.0, 0.0, 0.0, 0.0, 0.0]
position_ids=tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
        54, 55, 56, 57, 58, 59, 60, 61, 62, 63])
position_ids.shape=torch.Size([64])
rotary_emb.shape=torch.Size([1, 64, 1, 128])
[process_layer_prefill] causal_mask.shape= torch.Size([1, 1, 64, 512])

PREFILL - Input shapes:
  hidden_states: torch.Size([1, 64, 4096]), current_pos: tensor([0]), batch: 64
[forward_prefill.0] K_layer_cache.shape= torch.Size([8, 512, 128])
[forward_prefill.1] hidden_states.shape= torch.Size([1, 64, 4096])
[forward_prefill.1] query_states.shape= torch.Size([1, 32, 64, 128])
[forward_prefill.1] key_states.shape= torch.Size([1, 32, 512, 128])
[forward_prefill.1] value_states.shape= torch.Size([1, 32, 512, 128])
[forward_prefill.2] causal_mask.shape= torch.Size([1, 1, 64, 512])
[forward_prefill.2] attn_weights.shape= torch.Size([1, 32, 64, 512])
position_ids=tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
        54, 55, 56, 57, 58, 59, 60, 61, 62, 63])
position_ids.shape=torch.Size([64])
rotary_emb.shape=torch.Size([1, 64, 1, 128])
[process_layer_prefill] causal_mask.shape= torch.Size([1, 1, 64, 512])

PREFILL - Input shapes:
  hidden_states: torch.Size([1, 64, 4096]), current_pos: tensor([0]), batch: 64
[forward_prefill.0] K_layer_cache.shape= torch.Size([8, 512, 128])
[forward_prefill.1] hidden_states.shape= torch.Size([1, 64, 4096])
[forward_prefill.1] query_states.shape= torch.Size([1, 32, 64, 128])
[forward_prefill.1] key_states.shape= torch.Size([1, 32, 512, 128])
[forward_prefill.1] value_states.shape= torch.Size([1, 32, 512, 128])
[forward_prefill.2] causal_mask.shape= torch.Size([1, 1, 64, 512])
[forward_prefill.2] attn_weights.shape= torch.Size([1, 32, 64, 512])
Skipping MLP for last layer in prefill mode

[DEBUG] Prefill complete:
  Final position: 10
  Ready for generation starting at position: 10

[TIMING] Prefill phase:
  Tokens processed: 10
  Time taken: 0.19 seconds
  Speed: 51.94 tokens/second

[DEBUG] Starting generation phase from position 10

Generation step 0:
  Current position: 10
  Position IDs: tensor([10])
  Single causal mask shape: torch.Size([1, 1, 1, 512])
  Update mask shape: torch.Size([1, 1, 512, 1])
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([10])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([10])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([10]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([10])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([10]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.01552581787109375, 0.002685546875, -0.00751495361328125, 0.01910400390625, -0.01806640625, 0.008270263671875, -0.02587890625, -0.05950927734375, -0.004413604736328125, -0.0132293701171875]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([11])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([11])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([11]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([11])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([11]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.012603759765625, -0.025115966796875, -0.02813720703125, 0.0158843994140625, -0.00618743896484375, -0.03857421875, 0.0689697265625, -0.074951171875, 0.007671356201171875, 0.001094818115234375]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([12])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([12])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([12]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([12])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([12]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.0128631591796875, -0.010040283203125, 0.00322723388671875, 0.044189453125, 0.021728515625, -0.02593994140625, 0.00133514404296875, -0.07568359375, 0.0166015625, -0.036468505859375]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([13])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([13])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([13]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([13])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([13]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.00618743896484375, 0.0109100341796875, -0.036468505859375, -0.044036865234375, -0.0278778076171875, 0.01322174072265625, -0.04364013671875, -0.093505859375, 0.0174407958984375, 0.026397705078125]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([14])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([14])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([14]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([14])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([14]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.00091552734375, 0.006221771240234375, 0.04248046875, -0.00972747802734375, -0.041839599609375, -0.00775909423828125, -0.0054931640625, -0.03070068359375, -0.0162353515625, 0.00241851806640625]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([15])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([15])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([15]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([15])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([15]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.036285400390625, -0.0255279541015625, 0.04620361328125, 0.026611328125, -0.0135650634765625, 0.00665283203125, -0.01227569580078125, -0.04693603515625, 0.0266571044921875, 0.027252197265625]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([16])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([16])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([16]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([16])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([16]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.0292510986328125, 0.00946044921875, 0.000652313232421875, 0.038299560546875, -0.006565093994140625, -0.042938232421875, -0.0016632080078125, -0.06292724609375, 0.0188140869140625, 0.0087890625]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([17])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([17])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([17]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([17])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([17]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.0753173828125, -0.0188446044921875, -0.03424072265625, 0.019683837890625, -0.035919189453125, 0.027191162109375, 0.0186309814453125, -0.11761474609375, 0.0355224609375, -0.0110015869140625]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([18])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([18])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([18]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([18])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([18]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.01409149169921875, 0.032989501953125, -0.002841949462890625, -0.01549530029296875, -0.0138397216796875, -0.01184844970703125, 0.02801513671875, -0.023406982421875, -0.036163330078125, 0.00380706787109375]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([19])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([19])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([19]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([19])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([19]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.0279083251953125, 0.0543212890625, 0.0179901123046875, 0.00612640380859375, 0.0133056640625, 0.0164337158203125, -0.03497314453125, -0.09210205078125, -0.05596923828125, 0.0390625]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([20])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([20])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([20]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([20])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([20]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.0222930908203125, 0.006267547607421875, 0.0220947265625, 0.0140533447265625, -0.0035858154296875, -0.0071258544921875, -0.006072998046875, -0.0126495361328125, 0.024444580078125, 0.005504608154296875]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([21])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([21])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([21]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([21])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([21]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.04925537109375, -0.031402587890625, 0.055389404296875, 0.0205535888671875, 0.04034423828125, -0.01386260986328125, 0.004505157470703125, -0.0826416015625, -0.025054931640625, 0.0221710205078125]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([22])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([22])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([22]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([22])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([22]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.017425537109375, 0.026092529296875, 0.036376953125, -0.00762939453125, -0.01041412353515625, 0.0016937255859375, 0.0142059326171875, -0.08160400390625, -0.029205322265625, 0.00588226318359375]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([23])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([23])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([23]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([23])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([23]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.01898193359375, -0.037200927734375, 0.000194549560546875, 0.02142333984375, 0.04119873046875, 0.00145721435546875, -0.0103912353515625, -0.038116455078125, 0.00678253173828125, 0.00879669189453125]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([24])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([24])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([24]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([24])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([24]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.0088653564453125, 0.0236663818359375, 0.003448486328125, 0.002735137939453125, 0.03350830078125, -0.022735595703125, -0.0200347900390625, -0.0809326171875, -0.06365966796875, -0.01776123046875]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([25])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([25])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([25]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([25])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([25]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.0112762451171875, 0.0046539306640625, 0.00893402099609375, 0.004558563232421875, 0.0052947998046875, -0.0112152099609375, 0.00823974609375, -0.0631103515625, -0.006740570068359375, 0.0162506103515625]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([26])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([26])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([26]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([26])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([26]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.00780487060546875, -0.03045654296875, -0.004852294921875, -0.0009403228759765625, 0.031494140625, 0.02392578125, -0.0291900634765625, -0.08807373046875, 0.0272216796875, 0.049957275390625]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([27])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([27])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([27]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([27])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([27]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.0176239013671875, 0.00766754150390625, 0.02783203125, 0.0121307373046875, 0.0166015625, 0.0056610107421875, 0.019378662109375, -0.05230712890625, 0.00022792816162109375, 0.00962066650390625]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([28])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([28])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([28]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([28])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([28]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.01459503173828125, -0.0020694732666015625, 0.0171661376953125, 0.0270233154296875, -0.013336181640625, 0.0186614990234375, 0.0, -0.10040283203125, 0.002620697021484375, -0.0224456787109375]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([29])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([29])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([29]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([29])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([29]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-8.58306884765625e-05, 0.031768798828125, 0.003753662109375, 0.00579833984375, 0.0158538818359375, -0.0034637451171875, -0.00519561767578125, -0.0638427734375, -0.0280609130859375, 0.0190582275390625]

Generation step 20:
  Current position: 30
  Position IDs: tensor([30])
  Single causal mask shape: torch.Size([1, 1, 1, 512])
  Update mask shape: torch.Size([1, 1, 512, 1])
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([30])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([30])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([30]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([30])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([30]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.0306549072265625, 0.01291656494140625, -0.0232391357421875, 0.0106048583984375, 0.0144195556640625, -0.011749267578125, -0.03326416015625, -0.0631103515625, -0.005657196044921875, 0.001220703125]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([31])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([31])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([31]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([31])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([31]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.00301361083984375, -0.00321197509765625, 0.00498199462890625, 0.033294677734375, -0.01373291015625, 0.01459503173828125, 0.002044677734375, -0.12127685546875, -0.002685546875, -0.007080078125]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([32])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([32])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([32]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([32])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([32]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.0111083984375, 0.04437255859375, 0.0, 0.013671875, 0.0250244140625, -0.00769805908203125, -0.01233673095703125, -0.07244873046875, -0.031707763671875, 0.03228759765625]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([33])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([33])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([33]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([33])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([33]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.0261688232421875, 0.0185394287109375, -0.02197265625, 0.0159912109375, 0.00792694091796875, -0.0163726806640625, -0.039215087890625, -0.06591796875, 0.001373291015625, 0.0029754638671875]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([34])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([34])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([34]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([34])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([34]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.034271240234375, -0.01953125, -0.021392822265625, 0.07373046875, -0.058563232421875, -0.038726806640625, -0.018280029296875, -0.1156005859375, -0.04083251953125, 0.040252685546875]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([35])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([35])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([35]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([35])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([35]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.0254669189453125, -0.00652313232421875, 0.027099609375, 0.002796173095703125, 0.0236053466796875, 0.0029296875, 0.022613525390625, -0.1182861328125, 0.036407470703125, 0.0369873046875]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([36])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([36])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([36]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([36])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([36]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.01727294921875, 0.01534271240234375, -0.002201080322265625, -0.0053253173828125, -0.01019287109375, -0.024169921875, 0.00545501708984375, -0.07464599609375, 0.00823211669921875, -0.040496826171875]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([37])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([37])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([37]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([37])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([37]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.01013946533203125, 0.0147705078125, -0.0261383056640625, 0.0272674560546875, -0.003692626953125, -0.00269317626953125, -0.0102691650390625, -0.06298828125, -0.00640106201171875, -0.0148162841796875]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([38])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([38])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([38]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([38])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([38]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.009063720703125, -0.01480865478515625, -0.03289794921875, 0.02099609375, 0.03533935546875, 0.0048980712890625, -0.04296875, -0.0298309326171875, -0.02130126953125, -0.0124359130859375]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([39])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([39])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([39]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([39])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([39]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.010101318359375, -0.031494140625, -0.0015850067138671875, -0.0176239013671875, 0.006031036376953125, 0.014984130859375, 0.0157012939453125, -0.0631103515625, 0.021453857421875, 0.016357421875]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([40])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([40])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([40]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([40])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([40]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.019500732421875, 0.030609130859375, 0.03326416015625, 0.03643798828125, 0.006641387939453125, -0.0148468017578125, 0.001651763916015625, -0.039947509765625, -0.003047943115234375, 0.023834228515625]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([41])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([41])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([41]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([41])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([41]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.0153656005859375, -0.029998779296875, 0.00018310546875, 0.0311279296875, 0.01043701171875, -0.0052947998046875, -0.0251007080078125, -0.0355224609375, -0.0205230712890625, -0.01039886474609375]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([42])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([42])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([42]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([42])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([42]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.02923583984375, -0.01690673828125, -0.0062408447265625, 0.01255035400390625, -0.0196533203125, -0.03619384765625, 0.03997802734375, -0.07501220703125, -0.0197601318359375, -0.01009368896484375]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([43])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([43])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([43]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([43])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([43]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.001880645751953125, 0.043243408203125, -0.000919342041015625, 0.0469970703125, -0.026123046875, -0.01505279541015625, -0.0118255615234375, -0.10986328125, -0.0168609619140625, -0.0120849609375]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([44])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([44])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([44]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([44])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([44]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.04156494140625, -0.0001220703125, -0.0130462646484375, -0.023468017578125, 0.0086517333984375, -0.00255584716796875, 0.015899658203125, -0.0643310546875, -0.03155517578125, 0.046661376953125]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([45])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([45])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([45]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([45])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([45]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.00609588623046875, 0.023681640625, 0.00031280517578125, 0.01226806640625, 0.0357666015625, -0.001087188720703125, 0.02166748046875, -0.06109619140625, -0.0249786376953125, 0.01517486572265625]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([46])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([46])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([46]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([46])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([46]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.00276947021484375, -0.0141143798828125, -0.0193939208984375, -0.004634857177734375, 0.00917816162109375, -0.021820068359375, -0.0330810546875, -0.059814453125, 0.002964019775390625, 0.0206298828125]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([47])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([47])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([47]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([47])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([47]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.007904052734375, 0.01486968994140625, -0.0089111328125, -0.004974365234375, 0.0269622802734375, -0.020751953125, 0.0076446533203125, -0.046966552734375, -0.01104736328125, 0.0567626953125]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([48])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([48])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([48]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([48])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([48]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.01035308837890625, -0.016815185546875, -0.0290069580078125, 0.0200653076171875, 0.046112060546875, 0.00534820556640625, -0.056549072265625, -0.05377197265625, -0.0228424072265625, -0.016632080078125]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([49])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([49])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([49]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([49])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([49]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.00476837158203125, -0.03143310546875, -0.01256561279296875, -0.016357421875, 0.009033203125, 0.0113983154296875, 0.01387786865234375, -0.0830078125, 0.00885009765625, 0.02716064453125]

Generation step 40:
  Current position: 50
  Position IDs: tensor([50])
  Single causal mask shape: torch.Size([1, 1, 1, 512])
  Update mask shape: torch.Size([1, 1, 512, 1])
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([50])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([50])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([50]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([50])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([50]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.01727294921875, 0.0318603515625, 0.037109375, 0.038238525390625, 0.0101165771484375, -0.017059326171875, 0.0067901611328125, -0.05169677734375, -0.004547119140625, 0.01910400390625]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([51])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([51])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([51]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([51])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([51]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.007232666015625, 0.0043182373046875, 0.00620269775390625, 0.033966064453125, 0.026123046875, -0.0261383056640625, -0.01416015625, -0.062469482421875, -0.02716064453125, -0.020355224609375]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([52])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([52])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([52]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([52])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([52]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.0269775390625, -0.010284423828125, -0.0058135986328125, 0.021331787109375, -0.037994384765625, -0.02520751953125, -0.01435089111328125, -0.06744384765625, 0.0026226043701171875, 0.0090789794921875]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([53])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([53])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([53]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([53])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([53]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.0063934326171875, 0.031829833984375, -0.0087127685546875, 0.0208587646484375, -0.028594970703125, -0.0044097900390625, -0.024566650390625, -0.06622314453125, -0.0080413818359375, -0.0275726318359375]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([54])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([54])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([54]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([54])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([54]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.0190887451171875, -0.0004901885986328125, -0.0167083740234375, 0.0224761962890625, 0.01666259765625, -0.0137481689453125, 0.0313720703125, -0.0921630859375, -0.02886962890625, -0.0156707763671875]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([55])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([55])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([55]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([55])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([55]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.03497314453125, 0.010528564453125, -0.021820068359375, 0.041717529296875, -0.0049896240234375, -0.0241851806640625, 0.026947021484375, -0.055511474609375, -0.06884765625, -0.037933349609375]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([56])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([56])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([56]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([56])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([56]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.041168212890625, 0.025848388671875, -0.032440185546875, 0.07318115234375, -0.000263214111328125, -0.0211639404296875, -0.01058197021484375, -0.03912353515625, -0.033447265625, 0.001201629638671875]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([57])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([57])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([57]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([57])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([57]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.006771087646484375, -0.02362060546875, 0.00551605224609375, 0.04327392578125, -0.00386810302734375, 0.0123138427734375, -0.00455474853515625, -0.111328125, 0.0083160400390625, -0.0149383544921875]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([58])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([58])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([58]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([58])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([58]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.0118408203125, 0.03887939453125, 0.00225830078125, 0.01543426513671875, 0.01910400390625, -0.0075836181640625, -0.01239776611328125, -0.07415771484375, -0.024261474609375, 0.0236663818359375]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([59])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([59])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([59]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([59])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([59]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.03070068359375, 0.0203399658203125, -0.025115966796875, 0.0153961181640625, 0.005962371826171875, -0.014678955078125, -0.0369873046875, -0.0771484375, -0.000598907470703125, -0.00312042236328125]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([60])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([60])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([60]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([60])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([60]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.03289794921875, -0.015167236328125, -0.01483154296875, 0.084228515625, -0.05230712890625, -0.03668212890625, -0.0244293212890625, -0.12451171875, -0.03375244140625, 0.036468505859375]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([61])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([61])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([61]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([61])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([61]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.02117919921875, -0.0122528076171875, 0.01959228515625, 0.0282745361328125, -0.0233306884765625, -0.001220703125, -0.01336669921875, -0.044891357421875, 0.02001953125, 0.032379150390625]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([62])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([62])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([62]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([62])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([62]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.0279693603515625, 0.0110931396484375, -0.00439453125, 0.0643310546875, -0.030242919921875, -0.0281219482421875, -0.039337158203125, -0.072998046875, -0.0081329345703125, -0.0278167724609375]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([63])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([63])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([63]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([63])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([63]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.04736328125, -0.00571441650390625, -0.0330810546875, 0.03857421875, 0.033294677734375, -0.0496826171875, 0.002285003662109375, -0.06878662109375, -0.0313720703125, 0.017608642578125]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([64])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([64])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([64]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([64])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([64]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.013763427734375, 0.001514434814453125, 0.0023899078369140625, 0.034332275390625, -0.014739990234375, -0.0040283203125, -0.00725555419921875, -0.04638671875, -0.000698089599609375, 0.01343536376953125]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([65])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([65])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([65]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([65])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([65]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.039306640625, -0.012451171875, 0.03741455078125, 0.027496337890625, 0.02435302734375, -0.0152435302734375, -0.0150604248046875, -0.123046875, 0.0323486328125, 0.0321044921875]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([66])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([66])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([66]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([66])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([66]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.0328369140625, 0.0653076171875, 0.0210723876953125, -0.031341552734375, 0.0044708251953125, 0.004184722900390625, 0.000934600830078125, -0.027191162109375, -0.03167724609375, 0.018096923828125]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([67])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([67])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([67]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([67])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([67]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.001735687255859375, 0.020843505859375, 0.004131317138671875, 0.010528564453125, 0.01194000244140625, 0.0102081298828125, -0.002899169921875, -0.038421630859375, -0.01340484619140625, 0.00957489013671875]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([68])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([68])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([68]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([68])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([68]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.02459716796875, -0.002964019775390625, -0.00476837158203125, 0.030029296875, -0.038665771484375, -0.026611328125, -0.013275146484375, -0.0693359375, 0.0062408447265625, 0.006267547607421875]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([69])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([69])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([69]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([69])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([69]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.00968170166015625, 0.025238037109375, -0.0099639892578125, 0.0226898193359375, -0.0190582275390625, -0.00637054443359375, -0.0263671875, -0.06817626953125, 0.00304412841796875, -0.0240478515625]

Generation step 60:
  Current position: 70
  Position IDs: tensor([70])
  Single causal mask shape: torch.Size([1, 1, 1, 512])
  Update mask shape: torch.Size([1, 1, 512, 1])
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([70])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([70])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([70]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([70])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([70]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.021270751953125, 0.0293121337890625, 0.05291748046875, 0.034027099609375, 0.038238525390625, -0.000751495361328125, 0.022796630859375, -0.07861328125, 0.00321197509765625, 0.035614013671875]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([71])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([71])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([71]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([71])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([71]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.0251007080078125, 0.038177490234375, 0.03826904296875, 0.059814453125, -0.053955078125, -0.0044708251953125, -0.034637451171875, -0.136962890625, 0.02471923828125, 0.0274658203125]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([72])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([72])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([72]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([72])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([72]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.005077362060546875, 0.0284423828125, -0.00536346435546875, 0.0447998046875, -0.025665283203125, -0.016448974609375, -0.024871826171875, -0.049346923828125, 0.01418304443359375, 0.006317138671875]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([73])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([73])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([73]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([73])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([73]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.0654296875, -0.02227783203125, 0.02142333984375, -0.015655517578125, 0.0209503173828125, -0.013671875, -0.01427459716796875, -0.156982421875, -0.0391845703125, -0.01346588134765625]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([74])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([74])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([74]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([74])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([74]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.0181884765625, -0.0167694091796875, 0.0426025390625, 0.04010009765625, 0.009918212890625, 0.04296875, 0.003772735595703125, -0.1044921875, -0.00342559814453125, 0.0440673828125]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([75])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([75])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([75]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([75])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([75]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.00799560546875, 0.0023097991943359375, 0.01049041748046875, 0.006500244140625, 0.00568389892578125, -0.0074005126953125, -0.0202178955078125, -0.04901123046875, -0.005706787109375, 0.0099334716796875]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([76])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([76])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([76]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([76])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([76]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.073974609375, -0.02239990234375, 0.037017822265625, -0.00555419921875, 0.0152435302734375, -0.026214599609375, -0.018463134765625, -0.16796875, -0.02691650390625, -0.020172119140625]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([77])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([77])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([77]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([77])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([77]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.01751708984375, -0.0185699462890625, 0.0458984375, 0.04095458984375, 0.0136260986328125, 0.035400390625, -0.00321197509765625, -0.099853515625, -0.00396728515625, 0.04632568359375]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([78])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([78])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([78]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([78])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([78]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.00409698486328125, 0.004261016845703125, 0.0189666748046875, 0.0080413818359375, 0.00855255126953125, -0.008453369140625, -0.02197265625, -0.0386962890625, -0.00604248046875, 0.0142974853515625]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([79])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([79])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([79]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([79])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([79]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.0721435546875, -0.0220489501953125, 0.04022216796875, -0.004276275634765625, 0.016082763671875, -0.0283966064453125, -0.022796630859375, -0.1688232421875, -0.0246734619140625, -0.0224151611328125]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([80])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([80])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([80]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([80])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([80]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.017578125, -0.01763916015625, 0.04559326171875, 0.0421142578125, 0.011016845703125, 0.033477783203125, -0.004192352294921875, -0.10223388671875, -0.00420379638671875, 0.0462646484375]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([81])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([81])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([81]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([81])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([81]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.00400543212890625, 0.0052642822265625, 0.021026611328125, 0.0110321044921875, 0.0072479248046875, -0.008514404296875, -0.020965576171875, -0.03704833984375, -0.004302978515625, 0.016510009765625]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([82])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([82])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([82]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([82])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([82]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.0224456787109375, 0.00951385498046875, 0.033203125, 0.007537841796875, -0.042388916015625, 0.00839996337890625, 0.0372314453125, -0.115478515625, 0.0139923095703125, 0.020599365234375]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([83])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([83])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([83]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([83])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([83]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.015045166015625, 0.0088653564453125, -0.0528564453125, 0.020782470703125, 0.0153961181640625, 0.0052642822265625, 0.0400390625, -0.0282745361328125, -0.0330810546875, -0.02825927734375]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([84])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([84])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([84]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([84])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([84]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.008575439453125, -0.0036602020263671875, -0.04327392578125, 0.00774383544921875, 0.0017719268798828125, -0.0234832763671875, 0.017425537109375, -0.1107177734375, 0.01416015625, 0.0216827392578125]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([85])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([85])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([85]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([85])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([85]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.028778076171875, 0.012115478515625, -0.01108551025390625, 0.01824951171875, -0.006626129150390625, 0.01122283935546875, -0.0167236328125, -0.079345703125, 0.0161895751953125, 0.0178680419921875]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([86])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([86])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([86]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([86])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([86]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.0227813720703125, -0.0253143310546875, -0.0196685791015625, 0.0726318359375, -0.0465087890625, -0.0255126953125, -0.0260467529296875, -0.10400390625, -0.0115203857421875, 0.0177154541015625]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([87])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([87])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([87]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([87])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([87]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.04888916015625, -0.0264129638671875, 0.025543212890625, -0.019439697265625, -0.01104736328125, 0.00023651123046875, 0.004730224609375, -0.1055908203125, 0.037750244140625, 0.0227203369140625]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([88])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([88])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([88]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([88])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([88]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.0136566162109375, 0.0120086669921875, 0.003696441650390625, -0.011260986328125, -0.00395965576171875, -0.026580810546875, -0.00139617919921875, -0.07568359375, 0.020538330078125, -0.052398681640625]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([89])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([89])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([89]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([89])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([89]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.00855255126953125, 0.0130462646484375, -0.0249786376953125, 0.03009033203125, -0.0047454833984375, -0.001071929931640625, -0.0130767822265625, -0.060760498046875, -0.000530242919921875, -0.0188446044921875]

Generation step 80:
  Current position: 90
  Position IDs: tensor([90])
  Single causal mask shape: torch.Size([1, 1, 1, 512])
  Update mask shape: torch.Size([1, 1, 512, 1])
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([90])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([90])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([90]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([90])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([90]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.016326904296875, -0.0106964111328125, 0.00565338134765625, -0.0182647705078125, 0.03448486328125, -0.011962890625, -0.0479736328125, -0.0880126953125, -0.00890350341796875, -0.03009033203125]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([91])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([91])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([91]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([91])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([91]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.03802490234375, 0.00955963134765625, -0.0002665519714355469, 0.00814056396484375, -0.0028438568115234375, -0.030059814453125, 0.0009403228759765625, -0.0887451171875, 0.0143585205078125, 0.0007190704345703125]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([92])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([92])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([92]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([92])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([92]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.0306396484375, -0.0217132568359375, -0.01031494140625, 0.0709228515625, -0.04229736328125, -0.0092620849609375, -0.00885009765625, -0.1124267578125, -0.011383056640625, 0.0208282470703125]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([93])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([93])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([93]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([93])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([93]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.01313018798828125, -0.0013408660888671875, 0.021881103515625, 0.0111083984375, 0.063720703125, 0.00418853759765625, 0.006191253662109375, -0.059234619140625, 0.015716552734375, 0.0272216796875]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([94])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([94])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([94]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([94])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([94]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.0176544189453125, 0.0158538818359375, -0.005512237548828125, -0.0245361328125, -0.0294189453125, -0.015228271484375, -0.00487518310546875, -0.083251953125, 0.0548095703125, 0.0157318115234375]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([95])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([95])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([95]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([95])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([95]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.0008516311645507812, 0.01861572265625, 0.0303955078125, 0.006359100341796875, -0.008575439453125, -0.0102081298828125, 0.005886077880859375, -0.09405517578125, 0.01377105712890625, 0.00096893310546875]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([96])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([96])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([96]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([96])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([96]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.019561767578125, 0.032806396484375, -0.02349853515625, 0.031402587890625, -0.0270843505859375, 0.008026123046875, -0.001800537109375, -0.11846923828125, -0.00732421875, -0.0074615478515625]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([97])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([97])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([97]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([97])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([97]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.031646728515625, 0.0170440673828125, 0.004367828369140625, 0.0172882080078125, 0.0097808837890625, -0.01544189453125, 0.0006117820739746094, -0.063720703125, -0.018402099609375, 0.0181732177734375]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([98])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([98])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([98]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([98])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([98]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.06732177734375, 0.069091796875, -0.0304107666015625, 0.0231475830078125, -0.00872802734375, -0.00240325927734375, -0.013458251953125, -0.040679931640625, -0.023193359375, 0.0077056884765625]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([99])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([99])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([99]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([99])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([99]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.0055694580078125, 0.00954437255859375, -0.0088043212890625, 0.00650787353515625, -0.014190673828125, -0.004261016845703125, -0.0079193115234375, -0.058441162109375, -0.0034637451171875, 0.00653076171875]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([100])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([100])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([100]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([100])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([100]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.009735107421875, 0.01424407958984375, -0.054779052734375, 0.0001430511474609375, 0.022308349609375, 0.0071868896484375, 0.0286102294921875, -0.09130859375, 0.0022430419921875, -0.06219482421875]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([101])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([101])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([101]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([101])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([101]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.013824462890625, -0.008697509765625, 0.0027065277099609375, 0.01230621337890625, 0.041748046875, -0.01091766357421875, 0.023345947265625, -0.029083251953125, 0.022430419921875, 0.000751495361328125]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([102])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([102])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([102]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([102])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([102]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.03192138671875, 0.01629638671875, -0.00714111328125, 0.040740966796875, -0.041595458984375, 0.0218505859375, 0.0016078948974609375, -0.07257080078125, -0.002285003662109375, 0.061553955078125]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([103])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([103])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([103]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([103])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([103]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.00580596923828125, 0.0153045654296875, -0.0124359130859375, 0.0245361328125, -0.018646240234375, 0.017974853515625, -0.016998291015625, -0.11761474609375, -0.001926422119140625, 0.0022678375244140625]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([104])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([104])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([104]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([104])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([104]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.0121917724609375, 0.0233612060546875, -9.1552734375e-05, 0.01485443115234375, -0.013153076171875, -0.014678955078125, -0.0013866424560546875, -0.05224609375, -0.007617950439453125, 0.00392913818359375]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([105])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([105])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([105]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([105])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([105]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.0743408203125, -0.0174713134765625, 0.034423828125, 0.0047454833984375, 0.015777587890625, -0.024810791015625, -0.0311737060546875, -0.172119140625, -0.022186279296875, -0.0239105224609375]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([106])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([106])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([106]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([106])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([106]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.05792236328125, 0.049346923828125, 0.00525665283203125, -0.003093719482421875, -0.05816650390625, -0.00797271728515625, -0.007686614990234375, -0.1549072265625, -0.0321044921875, -0.004581451416015625]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([107])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([107])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([107]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([107])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([107]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.020965576171875, 9.5367431640625e-05, 0.008056640625, -0.0102081298828125, -0.025177001953125, 0.01319122314453125, -0.0159912109375, -0.06658935546875, 0.005786895751953125, 0.01019287109375]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([108])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([108])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([108]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([108])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([108]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.051544189453125, 0.0205230712890625, -0.029052734375, -0.0026397705078125, -0.0263824462890625, -0.01480865478515625, -0.026702880859375, -0.0947265625, 0.0226287841796875, 0.025238037109375]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([109])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([109])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([109]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([109])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([109]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.00579833984375, 0.007904052734375, 0.00731658935546875, 0.01557159423828125, 0.004566192626953125, -0.02569580078125, -0.00170135498046875, -0.06304931640625, 0.0041656494140625, -0.01381683349609375]

Generation step 100:
  Current position: 110
  Position IDs: tensor([110])
  Single causal mask shape: torch.Size([1, 1, 1, 512])
  Update mask shape: torch.Size([1, 1, 512, 1])
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([110])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([110])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([110]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([110])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([110]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.0244140625, -0.0433349609375, -0.0081634521484375, 0.084716796875, -0.0440673828125, -0.034210205078125, -0.0298309326171875, -0.097412109375, -0.005626678466796875, 0.0257568359375]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([111])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([111])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([111]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([111])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([111]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.0291900634765625, -0.0216064453125, 0.030059814453125, 0.01019287109375, 0.0100860595703125, -0.004856109619140625, 0.00994110107421875, -0.0980224609375, 0.042999267578125, 0.027069091796875]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([112])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([112])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([112]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([112])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([112]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.00753021240234375, 0.01593017578125, -0.047882080078125, 0.0274505615234375, -0.032745361328125, -0.020111083984375, 0.024566650390625, -0.07879638671875, 0.000736236572265625, -0.03118896484375]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([113])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([113])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([113]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([113])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([113]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.0186920166015625, -0.0051727294921875, -0.016693115234375, 0.00405120849609375, -0.0062103271484375, -0.007659912109375, -0.03778076171875, -0.10540771484375, 0.0011444091796875, -0.0017242431640625]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([114])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([114])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([114]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([114])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([114]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.0016937255859375, -0.0126800537109375, -0.0076904296875, 0.03472900390625, -0.00962066650390625, 0.002460479736328125, -0.0019378662109375, -0.12103271484375, 0.01288604736328125, -0.026275634765625]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([115])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([115])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([115]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([115])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([115]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.0177001953125, 0.039276123046875, 0.003253936767578125, 0.00872802734375, 0.019134521484375, -0.00754547119140625, -0.0100860595703125, -0.083251953125, -0.030792236328125, 0.022613525390625]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([116])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([116])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([116]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([116])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([116]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.030487060546875, 0.0202484130859375, -0.0225677490234375, 0.016387939453125, 0.00702667236328125, -0.0157928466796875, -0.041046142578125, -0.08953857421875, 0.005039215087890625, -0.003963470458984375]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([117])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([117])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([117]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([117])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([117]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.031707763671875, -0.014984130859375, -0.0179901123046875, 0.08154296875, -0.056610107421875, -0.03631591796875, -0.021820068359375, -0.124755859375, -0.030426025390625, 0.037261962890625]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([118])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([118])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([118]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([118])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([118]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.031463623046875, -0.00988006591796875, 0.0236053466796875, 0.0036773681640625, 0.01641845703125, -0.00250244140625, 0.0156707763671875, -0.1192626953125, 0.038787841796875, 0.038330078125]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([119])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([119])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([119]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([119])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([119]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.006626129150390625, 0.02166748046875, -0.048980712890625, 0.033050537109375, -0.033935546875, -0.01959228515625, 0.0204010009765625, -0.08575439453125, -0.001453399658203125, -0.033203125]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([120])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([120])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([120]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([120])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([120]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.01190185546875, 0.00485992431640625, -0.0162353515625, 0.0093841552734375, 0.00620269775390625, -0.0095367431640625, -0.04547119140625, -0.109619140625, -0.0019378662109375, -0.000446319580078125]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([121])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([121])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([121]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([121])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([121]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.0016946792602539062, -0.012359619140625, -0.00547027587890625, 0.03472900390625, -0.0099639892578125, 0.001186370849609375, -0.000244140625, -0.1240234375, 0.014068603515625, -0.0280914306640625]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([122])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([122])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([122]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([122])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([122]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.0160980224609375, 0.03729248046875, 0.003185272216796875, 0.0084686279296875, 0.02032470703125, -0.0086517333984375, -0.006465911865234375, -0.08544921875, -0.029815673828125, 0.022491455078125]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([123])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([123])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([123]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([123])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([123]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.034393310546875, 0.0270233154296875, -0.017059326171875, 0.0077056884765625, 0.0089263916015625, -0.01543426513671875, -0.0401611328125, -0.080322265625, 0.0022182464599609375, -0.005828857421875]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([124])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([124])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([124]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([124])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([124]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.032958984375, -0.015167236328125, -0.017669677734375, 0.0833740234375, -0.05572509765625, -0.03643798828125, -0.020172119140625, -0.127685546875, -0.035919189453125, 0.03875732421875]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([125])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([125])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([125]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([125])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([125]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.029632568359375, -0.009918212890625, 0.026580810546875, 0.00305938720703125, 0.01702880859375, -0.00316619873046875, 0.0171356201171875, -0.11834716796875, 0.0369873046875, 0.03814697265625]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([126])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([126])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([126]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([126])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([126]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.00499725341796875, 0.02398681640625, -0.050323486328125, 0.03326416015625, -0.0350341796875, -0.022186279296875, 0.01910400390625, -0.0887451171875, -0.003658294677734375, -0.02978515625]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([127])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([127])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([127]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([127])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([127]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [-0.0162811279296875, 0.006244659423828125, -0.0141754150390625, 0.0132904052734375, 0.00911712646484375, -0.01331329345703125, -0.0450439453125, -0.1143798828125, -0.00618743896484375, 0.0047149658203125]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([128])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([128])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([128]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([128])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([128]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.0007162094116210938, -0.01386260986328125, -0.004917144775390625, 0.03253173828125, -0.009490966796875, 0.001735687255859375, -0.0010528564453125, -0.124267578125, 0.01468658447265625, -0.02471923828125]
phimoeForCausalLM::forward called with input_ids: torch.Size([1, 1]), update_mask: torch.Size([1, 1, 512, 1]), position_ids: torch.Size([1]), causal_mask: torch.Size([1, 1, 1, 512]), current_pos: tensor([129])
PhimoeModel.forward - hidden_states shape: torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([129])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([129]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
normalized_states.shape=torch.Size([1, 1, 4096])
normalized_states.shape=torch.Size([1, 1, 4096])
position_ids=tensor([129])
position_ids.shape=torch.Size([1])
rotary_emb.shape=torch.Size([1, 1, 1, 128])

SINGLE TOKEN - Input shapes:
  hidden_states: torch.Size([1, 1, 4096]), current_pos: tensor([129]), q_len: 1
  After permute+unsqueeze: torch.Size([1, 4096, 1, 1])
  After projection:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
  After applying rotary:
    query_states: torch.Size([1, 32, 1, 128])
    key_states: torch.Size([1, 8, 1, 128])
    value_states: torch.Size([1, 8, 1, 128])
[TRACE]  B4 reshape attn_output shape=torch.Size([1, 32, 1, 128])
phimoeModel.forward - hidden_states last 10: [0.01666259765625, 0.03448486328125, 0.001903533935546875, 0.0088043212890625, 0.0222320556640625, -0.0097808837890625, -0.00685882568359375, -0.0831298828125, -0.0285491943359375, 0.0233001708984375]

[TIMING] Inference phase:
  Tokens generated: 120
  Time taken: 1.55 seconds
  Speed: 77.49 tokens/second

Prompt text:
<|user|> What is Apple Neural Engine? <|end|> <|assistant|>

Generated text:
aer assim IV fundamentally propulsion Seniority shortest possible adjusted permanently upside effects upside effects continuously monitored indirectly bogging style pic designsate bundles indirectly bogged backward compatibility petting upside effects continuity sandwiches formally inverted backward operators grape recurring recurring recurring probs such as continuously monitored longer than continu rotation speed upward omega- displacement thicknesses recruitment profilesd continuously synthesis upside effects continuously synthesis upside effects continuously synthesis upside effects
