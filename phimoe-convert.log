Checking dependencies...
Checking if macOS version is 15 or higher...
Checking if Python is installed...
Checking if pip is installed...
Checking if coremltools is installed via pip...
Checking if coremlcompiler is available...
Displaying coremlcompiler version...
coremlcompiler version: 3404.23.1
Checking if Python3 is installed...
Checking for model files in the provided directory: /Users/felixlin/models/Phi-3.5-MoE-instruct
Checking for supported architectures in config.json...
All dependencies are satisfied.
Checking dependencies...
Checking if macOS version is 15 or higher...
Checking if Python is installed...
Checking if pip is installed...
Checking if coremltools is installed via pip...
Checking if coremlcompiler is available...
Displaying coremlcompiler version...
coremlcompiler version: 3404.23.1
Checking if Python3 is installed...
Checking for model files in the provided directory: /Users/felixlin/models/Phi-3.5-MoE-instruct
Checking for supported architectures in config.json...
All dependencies are satisfied.
Skipping step 1: Converting Embeddings
Skipping step 2: Converting LM Head
Skipping step 3: Converting FFN
Step 4: Converting Prefill
Torch version 2.7.0 has not been tested with coremltools. You may run into unexpected errors. Torch 2.5.0 is the most recent version that has been tested.
/Users/felixlin/workspace-apple-silicon/anemll-bench/env-anemll-bench/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020
  warnings.warn(
Unrecognized keys in `rope_scaling` for 'rope_type'='longrope': {'long_mscale', 'short_mscale'}
This model has set a `original_max_position_embeddings` field, to be used together with `max_position_embeddings` to determine a scaling factor. Please set the `factor` field of `rope_scaling`with this ratio instead -- we recommend the use of this field over `original_max_position_embeddings`, as it is compatible with most model architectures.

Converting model from: /Users/felixlin/models/Phi-3.5-MoE-instruct
Output filename prefix: phimoe
Batch size: 64
Context length: 512
LUT quantization: 4 bits
Splitting into 2 chunks
Converting part(s): 2_prefill
Loading config from /Users/felixlin/models/Phi-3.5-MoE-instruct/config.json

Loaded model config:
  hidden_size: 4096
  vocab_size: 32064
Created lm_head8_1 through lm_head8_8

Loading pretrained weights...
Loading pretrained weights...
  0%|          | 0/36 [00:00<?, ?it/s] 58%|█████▊    | 21/36 [00:00<00:00, 203.36it/s]100%|██████████| 36/36 [00:00<00:00, 180.71it/s]
Traceback (most recent call last):
  File "/Users/felixlin/workspace-apple-silicon/Anemll/anemll/ane_converter/phimoe_converter.py", line 1118, in main
    test_conversion(
  File "/Users/felixlin/workspace-apple-silicon/Anemll/anemll/ane_converter/phimoe_converter.py", line 976, in test_conversion
    chunk_model = converter.convert_prefill(model, chunk_idx=i)
  File "/Users/felixlin/workspace-apple-silicon/Anemll/anemll/ane_converter/phimoe_converter.py", line 823, in convert_prefill
    traced_model = torch.jit.trace(
  File "/Users/felixlin/workspace-apple-silicon/anemll-bench/env-anemll-bench/lib/python3.9/site-packages/torch/jit/_trace.py", line 1002, in trace
    traced_func = _trace_impl(
  File "/Users/felixlin/workspace-apple-silicon/anemll-bench/env-anemll-bench/lib/python3.9/site-packages/torch/jit/_trace.py", line 696, in _trace_impl
    return trace_module(
  File "/Users/felixlin/workspace-apple-silicon/anemll-bench/env-anemll-bench/lib/python3.9/site-packages/torch/jit/_trace.py", line 1279, in trace_module
    module._c._create_method_from_trace(
  File "/Users/felixlin/workspace-apple-silicon/anemll-bench/env-anemll-bench/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/felixlin/workspace-apple-silicon/anemll-bench/env-anemll-bench/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/felixlin/workspace-apple-silicon/anemll-bench/env-anemll-bench/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1741, in _slow_forward
    result = self.forward(*input, **kwargs)
  File "/Users/felixlin/workspace-apple-silicon/Anemll/anemll/ane_converter/phimoe_converter.py", line 788, in forward
    return self.model.model(
  File "/Users/felixlin/workspace-apple-silicon/anemll-bench/env-anemll-bench/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/felixlin/workspace-apple-silicon/anemll-bench/env-anemll-bench/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/felixlin/workspace-apple-silicon/anemll-bench/env-anemll-bench/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1741, in _slow_forward
    result = self.forward(*input, **kwargs)
  File "/Users/felixlin/workspace-apple-silicon/Anemll/anemll/models/phimoe_model.py", line 1459, in forward
    hidden_states = self.process_layers(
  File "/Users/felixlin/workspace-apple-silicon/Anemll/anemll/models/phimoe_model.py", line 1427, in process_layers
    hidden_states = self.process_layer(
  File "/Users/felixlin/workspace-apple-silicon/Anemll/anemll/models/phimoe_model.py", line 1413, in process_layer
    return self.process_layer_prefill(layer_idx, hidden_states,  position_ids, causal_mask, current_pos, rotary_emb, layer_offset)
  File "/Users/felixlin/workspace-apple-silicon/Anemll/anemll/models/phimoe_model.py", line 1325, in process_layer_prefill
    hidden_states = hidden_states + layer.block_sparse_moe(post_attn)
  File "/Users/felixlin/workspace-apple-silicon/anemll-bench/env-anemll-bench/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/felixlin/workspace-apple-silicon/anemll-bench/env-anemll-bench/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/felixlin/workspace-apple-silicon/anemll-bench/env-anemll-bench/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1741, in _slow_forward
    result = self.forward(*input, **kwargs)
  File "/Users/felixlin/workspace-apple-silicon/Anemll/anemll/models/phimoe_model.py", line 628, in forward
    routing_weights, selected_experts = sparsemixer_simple(
TypeError: sparsemixer_simple() got an unexpected keyword argument 'jitter_eps'
Split lm_head weight into lm_head8_1.weight with shape torch.Size([4008, 4096, 1, 1])
Split lm_head weight into lm_head8_2.weight with shape torch.Size([4008, 4096, 1, 1])
Split lm_head weight into lm_head8_3.weight with shape torch.Size([4008, 4096, 1, 1])
Split lm_head weight into lm_head8_4.weight with shape torch.Size([4008, 4096, 1, 1])
Split lm_head weight into lm_head8_5.weight with shape torch.Size([4008, 4096, 1, 1])
Split lm_head weight into lm_head8_6.weight with shape torch.Size([4008, 4096, 1, 1])
Split lm_head weight into lm_head8_7.weight with shape torch.Size([4008, 4096, 1, 1])
Split lm_head weight into lm_head8_8.weight with shape torch.Size([4008, 4096, 1, 1])
Loading model.embed_tokens.weight with shape torch.Size([32064, 4096])
Moving model.embed_tokens.weight to embed_tokens.weight
(after loading emb/head) Missing keys: ['model.kv_cache_0', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.0.block_sparse_moe.gate.weight', 'model.layers.0.block_sparse_moe.experts.0.w1.weight', 'model.layers.0.block_sparse_moe.experts.0.w2.weight', 'model.layers.0.block_sparse_moe.experts.0.w3.weight', 'model.layers.0.block_sparse_moe.experts.1.w1.weight', 'model.layers.0.block_sparse_moe.experts.1.w2.weight', 'model.layers.0.block_sparse_moe.experts.1.w3.weight', 'model.layers.0.block_sparse_moe.experts.2.w1.weight', 'model.layers.0.block_sparse_moe.experts.2.w2.weight', 'model.layers.0.block_sparse_moe.experts.2.w3.weight', 'model.layers.0.block_sparse_moe.experts.3.w1.weight', 'model.layers.0.block_sparse_moe.experts.3.w2.weight', 'model.layers.0.block_sparse_moe.experts.3.w3.weight', 'model.layers.0.block_sparse_moe.experts.4.w1.weight', 'model.layers.0.block_sparse_moe.experts.4.w2.weight', 'model.layers.0.block_sparse_moe.experts.4.w3.weight', 'model.layers.0.block_sparse_moe.experts.5.w1.weight', 'model.layers.0.block_sparse_moe.experts.5.w2.weight', 'model.layers.0.block_sparse_moe.experts.5.w3.weight', 'model.layers.0.block_sparse_moe.experts.6.w1.weight', 'model.layers.0.block_sparse_moe.experts.6.w2.weight', 'model.layers.0.block_sparse_moe.experts.6.w3.weight', 'model.layers.0.block_sparse_moe.experts.7.w1.weight', 'model.layers.0.block_sparse_moe.experts.7.w2.weight', 'model.layers.0.block_sparse_moe.experts.7.w3.weight', 'model.layers.0.block_sparse_moe.experts.8.w1.weight', 'model.layers.0.block_sparse_moe.experts.8.w2.weight', 'model.layers.0.block_sparse_moe.experts.8.w3.weight', 'model.layers.0.block_sparse_moe.experts.9.w1.weight', 'model.layers.0.block_sparse_moe.experts.9.w2.weight', 'model.layers.0.block_sparse_moe.experts.9.w3.weight', 'model.layers.0.block_sparse_moe.experts.10.w1.weight', 'model.layers.0.block_sparse_moe.experts.10.w2.weight', 'model.layers.0.block_sparse_moe.experts.10.w3.weight', 'model.layers.0.block_sparse_moe.experts.11.w1.weight', 'model.layers.0.block_sparse_moe.experts.11.w2.weight', 'model.layers.0.block_sparse_moe.experts.11.w3.weight', 'model.layers.0.block_sparse_moe.experts.12.w1.weight', 'model.layers.0.block_sparse_moe.experts.12.w2.weight', 'model.layers.0.block_sparse_moe.experts.12.w3.weight', 'model.layers.0.block_sparse_moe.experts.13.w1.weight', 'model.layers.0.block_sparse_moe.experts.13.w2.weight', 'model.layers.0.block_sparse_moe.experts.13.w3.weight', 'model.layers.0.block_sparse_moe.experts.14.w1.weight', 'model.layers.0.block_sparse_moe.experts.14.w2.weight', 'model.layers.0.block_sparse_moe.experts.14.w3.weight', 'model.layers.0.block_sparse_moe.experts.15.w1.weight', 'model.layers.0.block_sparse_moe.experts.15.w2.weight', 'model.layers.0.block_sparse_moe.experts.15.w3.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.0.input_layernorm.bias', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.0.post_attention_layernorm.bias', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.1.block_sparse_moe.gate.weight', 'model.layers.1.block_sparse_moe.experts.0.w1.weight', 'model.layers.1.block_sparse_moe.experts.0.w2.weight', 'model.layers.1.block_sparse_moe.experts.0.w3.weight', 'model.layers.1.block_sparse_moe.experts.1.w1.weight', 'model.layers.1.block_sparse_moe.experts.1.w2.weight', 'model.layers.1.block_sparse_moe.experts.1.w3.weight', 'model.layers.1.block_sparse_moe.experts.2.w1.weight', 'model.layers.1.block_sparse_moe.experts.2.w2.weight', 'model.layers.1.block_sparse_moe.experts.2.w3.weight', 'model.layers.1.block_sparse_moe.experts.3.w1.weight', 'model.layers.1.block_sparse_moe.experts.3.w2.weight', 'model.layers.1.block_sparse_moe.experts.3.w3.weight', 'model.layers.1.block_sparse_moe.experts.4.w1.weight', 'model.layers.1.block_sparse_moe.experts.4.w2.weight', 'model.layers.1.block_sparse_moe.experts.4.w3.weight', 'model.layers.1.block_sparse_moe.experts.5.w1.weight', 'model.layers.1.block_sparse_moe.experts.5.w2.weight', 'model.layers.1.block_sparse_moe.experts.5.w3.weight', 'model.layers.1.block_sparse_moe.experts.6.w1.weight', 'model.layers.1.block_sparse_moe.experts.6.w2.weight', 'model.layers.1.block_sparse_moe.experts.6.w3.weight', 'model.layers.1.block_sparse_moe.experts.7.w1.weight', 'model.layers.1.block_sparse_moe.experts.7.w2.weight', 'model.layers.1.block_sparse_moe.experts.7.w3.weight', 'model.layers.1.block_sparse_moe.experts.8.w1.weight', 'model.layers.1.block_sparse_moe.experts.8.w2.weight', 'model.layers.1.block_sparse_moe.experts.8.w3.weight', 'model.layers.1.block_sparse_moe.experts.9.w1.weight', 'model.layers.1.block_sparse_moe.experts.9.w2.weight', 'model.layers.1.block_sparse_moe.experts.9.w3.weight', 'model.layers.1.block_sparse_moe.experts.10.w1.weight', 'model.layers.1.block_sparse_moe.experts.10.w2.weight', 'model.layers.1.block_sparse_moe.experts.10.w3.weight', 'model.layers.1.block_sparse_moe.experts.11.w1.weight', 'model.layers.1.block_sparse_moe.experts.11.w2.weight', 'model.layers.1.block_sparse_moe.experts.11.w3.weight', 'model.layers.1.block_sparse_moe.experts.12.w1.weight', 'model.layers.1.block_sparse_moe.experts.12.w2.weight', 'model.layers.1.block_sparse_moe.experts.12.w3.weight', 'model.layers.1.block_sparse_moe.experts.13.w1.weight', 'model.layers.1.block_sparse_moe.experts.13.w2.weight', 'model.layers.1.block_sparse_moe.experts.13.w3.weight', 'model.layers.1.block_sparse_moe.experts.14.w1.weight', 'model.layers.1.block_sparse_moe.experts.14.w2.weight', 'model.layers.1.block_sparse_moe.experts.14.w3.weight', 'model.layers.1.block_sparse_moe.experts.15.w1.weight', 'model.layers.1.block_sparse_moe.experts.15.w2.weight', 'model.layers.1.block_sparse_moe.experts.15.w3.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.1.input_layernorm.bias', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.1.post_attention_layernorm.bias', 'model.norm.weight', 'model.norm.bias']
Reshaped MoE router weight layers.8.block_sparse_moe.gate.weight from torch.Size([16, 4096]) to torch.Size([16, 4096, 1, 1])
Reshaped MoE router weight layers.9.block_sparse_moe.gate.weight from torch.Size([16, 4096]) to torch.Size([16, 4096, 1, 1])
Reshaped MoE router weight layers.21.block_sparse_moe.gate.weight from torch.Size([16, 4096]) to torch.Size([16, 4096, 1, 1])
Reshaped MoE router weight layers.22.block_sparse_moe.gate.weight from torch.Size([16, 4096]) to torch.Size([16, 4096, 1, 1])
Reshaped MoE router weight layers.31.block_sparse_moe.gate.weight from torch.Size([16, 4096]) to torch.Size([16, 4096, 1, 1])
Reshaped MoE router weight layers.6.block_sparse_moe.gate.weight from torch.Size([16, 4096]) to torch.Size([16, 4096, 1, 1])
Reshaped MoE router weight layers.7.block_sparse_moe.gate.weight from torch.Size([16, 4096]) to torch.Size([16, 4096, 1, 1])
Reshaped MoE weight layers.0.block_sparse_moe.experts.0.w1.weight from torch.Size([6400, 4096]) to torch.Size([6400, 4096, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.0.w2.weight from torch.Size([4096, 6400]) to torch.Size([4096, 6400, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.0.w3.weight from torch.Size([6400, 4096]) to torch.Size([6400, 4096, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.1.w1.weight from torch.Size([6400, 4096]) to torch.Size([6400, 4096, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.1.w2.weight from torch.Size([4096, 6400]) to torch.Size([4096, 6400, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.1.w3.weight from torch.Size([6400, 4096]) to torch.Size([6400, 4096, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.10.w1.weight from torch.Size([6400, 4096]) to torch.Size([6400, 4096, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.10.w2.weight from torch.Size([4096, 6400]) to torch.Size([4096, 6400, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.10.w3.weight from torch.Size([6400, 4096]) to torch.Size([6400, 4096, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.11.w1.weight from torch.Size([6400, 4096]) to torch.Size([6400, 4096, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.11.w2.weight from torch.Size([4096, 6400]) to torch.Size([4096, 6400, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.11.w3.weight from torch.Size([6400, 4096]) to torch.Size([6400, 4096, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.12.w1.weight from torch.Size([6400, 4096]) to torch.Size([6400, 4096, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.12.w2.weight from torch.Size([4096, 6400]) to torch.Size([4096, 6400, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.12.w3.weight from torch.Size([6400, 4096]) to torch.Size([6400, 4096, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.13.w1.weight from torch.Size([6400, 4096]) to torch.Size([6400, 4096, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.13.w2.weight from torch.Size([4096, 6400]) to torch.Size([4096, 6400, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.13.w3.weight from torch.Size([6400, 4096]) to torch.Size([6400, 4096, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.14.w1.weight from torch.Size([6400, 4096]) to torch.Size([6400, 4096, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.14.w2.weight from torch.Size([4096, 6400]) to torch.Size([4096, 6400, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.14.w3.weight from torch.Size([6400, 4096]) to torch.Size([6400, 4096, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.15.w1.weight from torch.Size([6400, 4096]) to torch.Size([6400, 4096, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.15.w2.weight from torch.Size([4096, 6400]) to torch.Size([4096, 6400, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.15.w3.weight from torch.Size([6400, 4096]) to torch.Size([6400, 4096, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.2.w1.weight from torch.Size([6400, 4096]) to torch.Size([6400, 4096, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.2.w2.weight from torch.Size([4096, 6400]) to torch.Size([4096, 6400, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.2.w3.weight from torch.Size([6400, 4096]) to torch.Size([6400, 4096, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.3.w1.weight from torch.Size([6400, 4096]) to torch.Size([6400, 4096, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.3.w2.weight from torch.Size([4096, 6400]) to torch.Size([4096, 6400, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.3.w3.weight from torch.Size([6400, 4096]) to torch.Size([6400, 4096, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.4.w1.weight from torch.Size([6400, 4096]) to torch.Size([6400, 4096, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.4.w2.weight from torch.Size([4096, 6400]) to torch.Size([4096, 6400, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.4.w3.weight from torch.Size([6400, 4096]) to torch.Size([6400, 4096, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.5.w1.weight from torch.Size([6400, 4096]) to torch.Size([6400, 4096, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.5.w2.weight from torch.Size([4096, 6400]) to torch.Size([4096, 6400, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.5.w3.weight from torch.Size([6400, 4096]) to torch.Size([6400, 4096, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.6.w1.weight from torch.Size([6400, 4096]) to torch.Size([6400, 4096, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.6.w2.weight from torch.Size([4096, 6400]) to torch.Size([4096, 6400, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.6.w3.weight from torch.Size([6400, 4096]) to torch.Size([6400, 4096, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.7.w1.weight from torch.Size([6400, 4096]) to torch.Size([6400, 4096, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.7.w2.weight from torch.Size([4096, 6400]) to torch.Size([4096, 6400, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.7.w3.weight from torch.Size([6400, 4096]) to torch.Size([6400, 4096, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.8.w1.weight from torch.Size([6400, 4096]) to torch.Size([6400, 4096, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.8.w2.weight from torch.Size([4096, 6400]) to torch.Size([4096, 6400, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.8.w3.weight from torch.Size([6400, 4096]) to torch.Size([6400, 4096, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.9.w1.weight from torch.Size([6400, 4096]) to torch.Size([6400, 4096, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.9.w2.weight from torch.Size([4096, 6400]) to torch.Size([4096, 6400, 1, 1])... (& more layers...
Reshaped MoE weight layers.0.block_sparse_moe.experts.9.w3.weight from torch.Size([6400, 4096]) to torch.Size([6400, 4096, 1, 1])... (& more layers...
Reshaped MoE router weight layers.0.block_sparse_moe.gate.weight from torch.Size([16, 4096]) to torch.Size([16, 4096, 1, 1])
Reshaped layers.0.self_attn.k_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])... (& more layers...
Keeping o_proj weights as 2D: layers.0.self_attn.o_proj.weight shape torch.Size([4096, 4096])... (& more layers...
Reshaped layers.0.self_attn.q_proj.weight from torch.Size([4096, 4096]) to torch.Size([4096, 4096, 1, 1])... (& more layers...
Reshaped layers.0.self_attn.v_proj.weight from torch.Size([1024, 4096]) to torch.Size([1024, 4096, 1, 1])... (& more layers...
Reshaped MoE router weight layers.1.block_sparse_moe.gate.weight from torch.Size([16, 4096]) to torch.Size([16, 4096, 1, 1])
Reshaped MoE router weight layers.23.block_sparse_moe.gate.weight from torch.Size([16, 4096]) to torch.Size([16, 4096, 1, 1])
Reshaped MoE router weight layers.24.block_sparse_moe.gate.weight from torch.Size([16, 4096]) to torch.Size([16, 4096, 1, 1])
Reshaped MoE router weight layers.29.block_sparse_moe.gate.weight from torch.Size([16, 4096]) to torch.Size([16, 4096, 1, 1])
Reshaped MoE router weight layers.30.block_sparse_moe.gate.weight from torch.Size([16, 4096]) to torch.Size([16, 4096, 1, 1])
Reshaped MoE router weight layers.27.block_sparse_moe.gate.weight from torch.Size([16, 4096]) to torch.Size([16, 4096, 1, 1])
Reshaped MoE router weight layers.28.block_sparse_moe.gate.weight from torch.Size([16, 4096]) to torch.Size([16, 4096, 1, 1])
Reshaped MoE router weight layers.18.block_sparse_moe.gate.weight from torch.Size([16, 4096]) to torch.Size([16, 4096, 1, 1])
Reshaped MoE router weight layers.19.block_sparse_moe.gate.weight from torch.Size([16, 4096]) to torch.Size([16, 4096, 1, 1])
Reshaped MoE router weight layers.2.block_sparse_moe.gate.weight from torch.Size([16, 4096]) to torch.Size([16, 4096, 1, 1])
Reshaped MoE router weight layers.3.block_sparse_moe.gate.weight from torch.Size([16, 4096]) to torch.Size([16, 4096, 1, 1])
Reshaped MoE router weight layers.12.block_sparse_moe.gate.weight from torch.Size([16, 4096]) to torch.Size([16, 4096, 1, 1])
Reshaped MoE router weight layers.13.block_sparse_moe.gate.weight from torch.Size([16, 4096]) to torch.Size([16, 4096, 1, 1])
Reshaped MoE router weight layers.14.block_sparse_moe.gate.weight from torch.Size([16, 4096]) to torch.Size([16, 4096, 1, 1])
Reshaped MoE router weight layers.15.block_sparse_moe.gate.weight from torch.Size([16, 4096]) to torch.Size([16, 4096, 1, 1])
Reshaped MoE router weight layers.25.block_sparse_moe.gate.weight from torch.Size([16, 4096]) to torch.Size([16, 4096, 1, 1])
Reshaped MoE router weight layers.26.block_sparse_moe.gate.weight from torch.Size([16, 4096]) to torch.Size([16, 4096, 1, 1])
Reshaped MoE router weight layers.20.block_sparse_moe.gate.weight from torch.Size([16, 4096]) to torch.Size([16, 4096, 1, 1])
Reshaped MoE router weight layers.16.block_sparse_moe.gate.weight from torch.Size([16, 4096]) to torch.Size([16, 4096, 1, 1])
Reshaped MoE router weight layers.17.block_sparse_moe.gate.weight from torch.Size([16, 4096]) to torch.Size([16, 4096, 1, 1])
Reshaped MoE router weight layers.4.block_sparse_moe.gate.weight from torch.Size([16, 4096]) to torch.Size([16, 4096, 1, 1])
Reshaped MoE router weight layers.5.block_sparse_moe.gate.weight from torch.Size([16, 4096]) to torch.Size([16, 4096, 1, 1])
Reshaped MoE router weight layers.10.block_sparse_moe.gate.weight from torch.Size([16, 4096]) to torch.Size([16, 4096, 1, 1])
Reshaped MoE router weight layers.11.block_sparse_moe.gate.weight from torch.Size([16, 4096]) to torch.Size([16, 4096, 1, 1])
Pretrained weights loaded successfully
Note: The following expected buffers were initialized:
  - kv_cache_0
  - layers.0.self_attn.rotary_emb.inv_freq
  - layers.1.self_attn.rotary_emb.inv_freq

Converting chunk 1/2

Converting transformer prefill mode...
Processing chunk 1/2 (layers 0 to 0)
GetTransformerStates part=2_prefill ENABLE_UNIFIED_CACHE=True num_layers_this_part=4 model.config.num_hidden_layers=2
Tracing prefill model...
Error during prefill conversion: sparsemixer_simple() got an unexpected keyword argument 'jitter_eps'

Error during conversion: sparsemixer_simple() got an unexpected keyword argument 'jitter_eps'
Error in step 4: Converting Prefill
